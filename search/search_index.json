{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MiCADO","text":"<p>Originally developed in Project COLA. MiCADOscale, or simply MiCADO, is currently used across multiple European projects. Development is ongoing at this github repository.</p>"},{"location":"#about","title":"About","text":"<p>MiCADO is a generic execution and auto-scaling framework for OCI containers, orchestrated by Kubernetes. It supports autoscaling at two levels. At virtual machine (VM) level, a built-in Kubernetes cluster is dynamically extended or reduced by adding/removing cloud virtual machines. At container level, the number of replicas tied to a specific Kubernetes Deployment can be increased/decreased.</p> <p>MiCADO expects a TOSCA-based Application Description Template (ADT) to be submitted containing these sections:</p> <ol> <li>Definition of the individual applications making up a Kubernetes Deployment,</li> <li>Specification of the virtual machine and</li> <li>(optional) Implementation of policies for scaling and monitoring VM and/or application.</li> </ol> <p>The format of the ADT for MiCADO is detailed later.</p> <p>To use MiCADO, first the MiCADO core services must be deployed on a virtual machine (the MiCADO Control Plane), usually by an Ansible playbook. The MiCADO Control Plane is configured as a Kubernetes Control Plane and installs a container runtime, a cloud orchestrator (Occopus or Terraform), Prometheus, Policy Keeper and TOSCASubmitter to realise the control loops. During operation MiCADO workers (realised on dynamically provisioned VMs or Edges) are instantiated on demand which deploy Prometheus Node Exporter and expose the metrics-server so Grafana can present metrics. The newly instantiated MiCADO workers join the Kubernetes cluster managed by the MiCADO Control Plane.</p>"},{"location":"arch/","title":"Architecture","text":"<p>MiCADO brings together the best in cloud native to empower users with the latest technology, all without the need for a degree in cloud native computing.</p> <p></p>"},{"location":"adt/","title":"Application Description Template","text":"<p>MiCADO executes applications described by Application Description Templates (ADT). The ADT is based on the TOSCA Specification and is described in detail in this section. Some familiarity with YAML is helpful.</p> <p>An ADT describes the following aspects of an application, which are documented individually:</p> <ul> <li>Containers</li> <li>Volumes &amp; Configs</li> <li>Virtual Machines</li> <li>Monitoring</li> <li>Scaling</li> <li>Networking</li> <li>Secrets</li> </ul>"},{"location":"adt/#overview","title":"Overview","text":"<ul> <li> <p>Application Description Templates are the domain specific language of MiCADO.</p> </li> <li> <p>They describe the container, virtual machine and scaling/security policies that make up an application deployment.</p> </li> <li> <p>These different sections of the ADT get translated and orchestrated by the relevant underlying tools in MiCADO.</p> </li> </ul>"},{"location":"adt/#a-quick-look","title":"A quick look...","text":"<p>ADTs are based on TOSCA, written in YAML and you can start writing one like this:</p> <pre><code>tosca_definitions_version: tosca_simple_yaml_1_3\n\nimports:\n- https://github.com/micado-scale/tosca/main/micado_types.yaml\n\ndescription: ADT for stressng on EC2\n</code></pre> <p>Next up, the <code>topology_template</code> is where you describe your containers, virtual machines and policies, beginning with container and virtual machines under <code>node_templates</code>  like so:</p> <pre><code>topology_template:\nnode_templates:\n\napp-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: uowcpc/nginx:v1.2\ninterfaces:\nKubernetes:\ncreate:\ninputs:\nkind: Deployment\n\n\nworker-virtualmachine:\ntype: tosca.nodes.MiCADO.EC2.Compute\nproperties:\ninstance_type: t2.small\ninterfaces:\nTerraform:\ncreate:\n</code></pre>"},{"location":"adt/#a-closer-look","title":"A closer look...","text":"<ul> <li> <p>Looking above, you can see we use a specific <code>type</code> to let our adaptors know what they are looking at.</p> </li> <li> <p>You can think of <code>properties</code> as providing the options and parameters specific to the basic resource that you want to orchestate (above, the Docker container, and the EC2 compute instance).</p> </li> <li> <p>The <code>interfaces</code> section, on the other hand, specifies the orchestrator that should be used to manage the defined resouce, with the option to pass in options and parameters for that tool (above, Kubernetes and Terraform).</p> </li> </ul>"},{"location":"adt/#main-sections","title":"Main sections","text":"<p>The main sections of an ADT are documented below.</p> <code>tosca_definitions_version</code> <p>string. Default: <code>tosca_simple_yaml_1_3</code>. The version of TOSCA to work with. v1.3 by default.</p> <code>imports</code> <p>string[ ]. List of urls pointing to custom TOSCA types. The default url points to the custom types defined for MiCADO. Please, do not modify this url.</p> <code>topology_template</code> <p>map. The body of the application description.</p> <code>node_templates</code> <p>map. Definitions of application containers and auxiliary components such as volumes and virtual machines.</p> <code>policies</code> <p>map. Scaling &amp; metric policies.</p> <code>types</code> <p>This section is used to optionally define additional detailed types which can be referenced in the topology_template section to benefit from abstraction. Under policy_types: for example, complex scaling logic can be defined here, then referenced in the policies section above</p>"},{"location":"adt/containers/","title":"Describing Containers","text":"<ul> <li>The Basics (this page)</li> <li>Services and container communication</li> <li>Exposing containers externally</li> <li>Volumes and Configs</li> <li>Multi-Container Pods (Sidecars)</li> <li>Special MiCADO/Kubernetes Types</li> </ul>"},{"location":"adt/containers/#getting-started","title":"Getting Started","text":"<p>This page will get you started with your first container. The links above will point you to more specific information, once you are ready for it.</p>"},{"location":"adt/containers/#properties-and-interfaces","title":"Properties and Interfaces","text":"<p>In case you missed it, there is a clear distinction between <code>properties</code> and <code>interfaces</code>. For containers specifically:</p> <ul> <li>We use <code>properties</code> to define options and parameters that are general   enough that they could be applied to any Docker container, regardless of the orchestrator.</li> <li>We use <code>inputs</code> under <code>interfaces</code> to indicate that we want to overwrite   the fields that one would find in a generated Kubernetes manifest</li> </ul> <p>Note</p> <p>Unless you've been explicit, MiCADO will try to orchestrate your container as a Kubernetes Deployment.</p>"},{"location":"adt/containers/#the-bare-minimum","title":"The bare minimum","text":"<p>At a minimum, you'll need a container image in DockerHub or another container registry that you want to use. Here's a minimum working example:</p> <pre><code>app-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: nginx\ninterfaces:\nKubernetes:\ncreate:\n</code></pre> <p>If you're interested, see how the generated Kubernetes manifest will look by opening the box below.</p> Kubernetes Manifest <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: app-container\nlabels:\napp.kubernetes.io/name: app-container\napp.kubernetes.io/instance: mwe\napp.kubernetes.io/managed-by: micado\napp.kubernetes.io/version: latest\nspec:\ntemplate:\nmetadata:\nlabels:\napp.kubernetes.io/name: app-container\napp.kubernetes.io/instance: mwe\napp.kubernetes.io/managed-by: micado\napp.kubernetes.io/version: latest\nspec:\ncontainers:\n- image: nginx\nname: app-container\nselector:\nmatchLabels:\napp.kubernetes.io/name: app-container\napp.kubernetes.io/instance: mwe\napp.kubernetes.io/managed-by: micado\napp.kubernetes.io/version: latest\n</code></pre> <p>For those familiar with Kubernetes, this mounts a single container (<code>nginx:latest</code>) inside a Pod owned by a Deployment using a set of labels based on Kubernetes recommendations. For those not familiar - this minimum working example creates a scalable unit of your specified container.</p> <p>Warning</p> <p>Note that because Kubernetes does not support underscores in resources names, neither do we.</p>"},{"location":"adt/containers/#choosing-a-host","title":"Choosing a host","text":"<p>Often, you'll want to be sure that your container runs on a specific node (virtual machine) within your deployment. You can accomplish this by referencing the virtual machine node in the requirements section of your container description, like so:</p> <pre><code>vm-node:\ntype: tosca.nodes.MiCADO.EC2.Compute\n...(truncated)...\n\napp-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: nginx\nrequirements:\n- host: vm-node\ninterfaces:\nKubernetes:\ncreate:\n</code></pre> <p>In Kubernetes-speak, this adds a NodeAffinity to the generated manifest. See the box below if you're interested in how it would look.</p> Kubernetes manifest <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: app-container\nlabels:\napp.kubernetes.io/name: app-container\napp.kubernetes.io/instance: affinity\napp.kubernetes.io/managed-by: micado\napp.kubernetes.io/version: latest\nspec:\ntemplate:\nmetadata:\nlabels:\napp.kubernetes.io/name: app-container\napp.kubernetes.io/instance: affinity\napp.kubernetes.io/managed-by: micado\napp.kubernetes.io/version: latest\nspec:\ncontainers:\n- image: nginx      affinity:\nnodeAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\nnodeSelectorTerms:\n- matchExpressions:\n- key: micado.eu/node_type\noperator: In\nvalues:\n- vm-node\nselector:\nmatchLabels:\napp.kubernetes.io/name: app-container\napp.kubernetes.io/instance: affinity\napp.kubernetes.io/managed-by: micado\napp.kubernetes.io/version: latest\n</code></pre>"},{"location":"adt/containers/#more-properties","title":"More properties","text":"<p>You will likely need to provide more customisation for your container than simply specifying an image. The available list of properties is available in the API Reference section.</p>"},{"location":"adt/containers/#switching-it-up","title":"Switching it up","text":"<p>If you need something more specific than the basic Deployment workload, you'll have to say so using <code>inputs</code>. Here's how it would look:</p> <pre><code>app-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: nginx\ninterfaces:\nKubernetes:\ncreate:\ninputs:\nkind: DaemonSet\n</code></pre> <p>The DaemonSet workload creates a replica of the container on each active node, like a global service in Docker Swarm</p> <p>We support all of the Workload Controllers, though to get certain workloads to validate, you'll have to provide further parameters under <code>inputs</code>.</p>"},{"location":"adt/containers/#more-inputs","title":"More inputs","text":"<p>If you're comfortable with Kubernetes manifests, you can fully customise the final generated manifest by using <code>inputs</code>. There's no definitive list of available options - you are only restricted by what's in the Kubernetes API. Here's an example with StatefulSets:</p> <pre><code>app-stateful:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: nginx\ninterfaces:\nKubernetes:\ncreate:\ninputs:\nkind: StatefulSet\nmetadata:\nlabels:\ninteresting_label: info\nspec:\nserviceName: app-stateful\nupdateStrategy:\ntype: RollingUpdate\ntemplate:\nspec:\nhostNetwork: True\n</code></pre> <p>Here we're adding lots of Kubernetes specific customisation, right down to the level of the PodSpec. See the generated manifest in the box below. Notice that the container we've described using <code>properties</code> is still at the core of this StatefulSet.</p> Kubernetes manifest <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\nlabels:\ninteresting_label: info\napp.kubernetes.io/name: app-stateful\napp.kubernetes.io/instance: stateful-ex\napp.kubernetes.io/managed-by: micado\napp.kubernetes.io/version: latest\nname: app-stateful\nspec:\nserviceName: app-stateful\nupdateStrategy:\ntype: RollingUpdate\ntemplate:\nspec:\ncontainers:\n- image: nginx\nname: app-stateful\nhostNetwork: true\nmetadata:\nlabels:\napp.kubernetes.io/name: app-stateful\napp.kubernetes.io/instance: stateful-ex\napp.kubernetes.io/managed-by: micado\napp.kubernetes.io/version: latest\nselector:\nmatchLabels:\napp.kubernetes.io/name: app-stateful\napp.kubernetes.io/instance: stateful-ex\napp.kubernetes.io/managed-by: micado\napp.kubernetes.io/version: latest\n</code></pre>"},{"location":"adt/containers/#next-up-services-and-container-communication","title":"Next up: Services and Container Communication","text":""},{"location":"adt/containers/custom/","title":"Describing Containers","text":"<ul> <li>The Basics</li> <li>Services and container communication</li> <li>Exposing containers externally</li> <li>Volumes and Configs</li> <li>Multi-Container Pods (Sidecars)</li> <li>Special MiCADO/Kubernetes Types (this page)</li> </ul>"},{"location":"adt/containers/custom/#stripping-away-some-complexity","title":"Stripping away some complexity","text":"<p>You should now have a stronger understanding of TOSCA and our ADTs. To save you some typing when you're preparing your next ADT, we have pre-prepared a number of TOSCA types for Kubernetes in MiCADO that we hope you find convenient.</p> <p>These TOSCA types either have default interfaces, default properties, or both - meaning that you won't have to write them out again unless you need to overwrite them.</p> <p>You can always find the current custom TOSCA types for Kubernetes in the main TOSCA repository. Here are four of our favourites:</p>"},{"location":"adt/containers/custom/#the-deployment-type","title":"The Deployment type","text":"<p><code>tosca.nodes.MiCADO.Container.Application.Docker.Deployment</code></p> <p>We use plain Kubernetes Deployments all the time, so we made a type that includes a Kubernetes interface by default.</p> <p>Use it like this:</p> <pre><code>my-easy-deployment:\ntype: tosca.nodes.MiCADO.Container.Application.Docker.Deployment\nproperties:\nimage: uowcpc/nginx:v1.2\nrequirements:\n- host: my-host\n</code></pre> <p>No need to define any interfaces here, that's taken care of by the type!</p> <p>There are also similar types for DaemonSet and StatefulSet</p>"},{"location":"adt/containers/custom/#the-emptydir-volume-type","title":"The EmptyDir Volume type","text":"<p><code>tosca.nodes.MiCADO.Container.Volume.EmptyDir</code></p> <p>Again, we use these alot, so we created a type for them.</p> <p>Use it like this:</p> <pre><code>my-easy-deployment:\ntype: tosca.nodes.MiCADO.Container.Application.Docker.Deployment\nproperties:\nimage: uowcpc/nginx:v1.2\nrequirements:\n- volume: my-empty\n\nmy-empty:\ntype: tosca.nodes.MiCADO.Container.Volume.EmptyDir\n</code></pre> <p>No properties, no interfaces, just two lines!</p>"},{"location":"adt/containers/custom/#the-nfs-volume-type","title":"The NFS Volume type","text":"<p><code>tosca.nodes.MiCADO.Container.Volume.NFS</code></p> <p>With this type we worked a bit of magic, and moved the NFS <code>server</code> and <code>path</code> options up out of <code>inputs</code> and into <code>properties</code>.</p> <p>Use it like this:</p> <pre><code>my-easy-deployment:\ntype: tosca.nodes.MiCADO.Container.Application.Docker.Deployment\nproperties:\nimage: uowcpc/nginx:v1.2\nrequirements:\n- volume: my-nfs\n\nmy-nfs:\ntype: tosca.nodes.MiCADO.Container.Volume.NFS\nproperties:\npath: /nfs/share\nserver: 10.0.0.2\n</code></pre> <p>The <code>path</code> property here still serves as the default mount point in the container, if you don't specify one otherwise.</p> <p>There is a similar type for HostPath!</p>"},{"location":"adt/containers/custom/#the-configmap-type","title":"The ConfigMap type","text":"<p><code>tosca.nodes.MiCADO.Container.Config.ConfigMap</code></p> <p>We did a similar thing to above, and moved the <code>data</code> and <code>binaryData</code> options up out of <code>inputs</code> and into <code>properties</code>.</p> <p>Use it like this:</p> <pre><code>my-easy-deployment:\ntype: tosca.nodes.MiCADO.Container.Application.Docker.Deployment\nproperties:\nimage: uowcpc/nginx:v1.2\nenvFrom:\n- configMapRef:\nname: my-config\n\nmy-config:\ntype: tosca.nodes.MiCADO.Container.Config.ConfigMap\nproperties:\ndata:\nsome: interesting_data\n</code></pre> <p>Enjoy!</p>"},{"location":"adt/containers/custom/#thats-all-see-the-api-reference-for-full-details","title":"That's all! See the API Reference for full details","text":""},{"location":"adt/containers/expose/","title":"Describing Containers","text":"<ul> <li>The Basics</li> <li>Services and container communication</li> <li>Exposing containers externally (this page)</li> <li>Volumes and Configs</li> <li>Multi-Container Pods (Sidecars)</li> <li>Special MiCADO/Kubernetes Types</li> </ul>"},{"location":"adt/containers/expose/#exposing-containers-externally","title":"Exposing containers externally","text":"<p>We've seen how services enable communication between our containers inside the cluster. Now we'll look at how we can expose containers (actually Pods) to allow ingress from outside the cluster.</p> <p>Before you start, make sure the firewall rules for your cloud instances are configured to allow ingress on the ports you choose.</p>"},{"location":"adt/containers/expose/#nodeport-service","title":"NodePort Service","text":"<p>NodePort is a special type of Kubernetes service that exposes our container at a random high number port, by default in the range 30000-32767. That port can be accessed on any node in the cluster. Here's how it looks in the ADT:</p> <pre><code>web-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: nginx\nports:\n- port: 80\ntype: NodePort\ninterfaces:\nKubernetes:\ncreate:\n</code></pre> <p>Since we're creating a service, we'll also get a ClusterIP and the container will be available internally at <code>web-container:80</code>.</p> <p>When a random port doesn't cut it, we can provide our own (so long as it falls in the 30000-32767 range) like so: <pre><code>...\nproperties:\nimage: nginx\nports:\n- port: 80\nnodePort: 30080\n</code></pre></p> <p>Note we forgot <code>type: NodePort</code> here. MiCADO sees a nodePort defined, so it fills this in for us.</p> <p>We can now reach port 80 of the NGINX container by pointing to <code>:30080</code> on any node in the cluster. Since we generally know the IP of the MiCADO Control Plane, the easiest endpoint would be <code>ip.of.micado.master:30080</code>.</p> <p>Info</p> <p>If multiple replicas of a container exist, Kubernetes will generally apply a round-robin technique for deciding which container to route a request to.</p>"},{"location":"adt/containers/expose/#hostport","title":"HostPort","text":"<p>Sometimes, the port range of NodePort isn't convenient. Using HostPort, we can expose our container outside the cluster using any available port number. However, since it's binding to the host, the container can only be accessed on the node where it is running. Here's how it looks in the ADT:</p> <pre><code>web-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: nginx\nports:\n- hostPort: 80\ncontainerPort: 80    interfaces:\nKubernetes:\ncreate:\n</code></pre> <p>Here, HostPort indicates the port on the node, and ContainerPort indicates the port in the container. </p> <p>Port 80 of the container defined in the above snippet can be accessed at <code>ip.of.host.node:80</code>.</p>"},{"location":"adt/containers/expose/#services-and-not-services","title":"Services and not services","text":"<p>HostPort is not a Kubernetes Service. So - under the ports key, we cannot put the <code>containerPort</code>/<code>hostPort</code> options in the same list item as Service options like <code>port</code>/<code>targetPort</code>. </p> <p>Doing something like the following, however, is perfectly valid:</p> <pre><code>web-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: nginx\nports:\n- containerPort: 80\nhostPort: 80\n- port: 8080\ntargetPort: 80\ninterfaces:\nKubernetes:\ncreate:\n</code></pre> <p>Inside the cluster, other containers can reach this one at <code>web-container:8080</code> Outside the cluster it is served at <code>ip.of.host.node:80</code></p>"},{"location":"adt/containers/expose/#next-up-volumes-and-configs","title":"Next up: Volumes and Configs","text":""},{"location":"adt/containers/services/","title":"Describing Containers","text":"<ul> <li>The Basics</li> <li>Services and container communication (this page)</li> <li>Exposing containers externally</li> <li>Volumes and Configs</li> <li>Multi-Container Pods (Sidecars)</li> <li>Special MiCADO/Kubernetes Types</li> </ul>"},{"location":"adt/containers/services/#services-and-container-communication","title":"Services and container communication","text":"<p>Permitting communication between your containers (more specifically - your Pods) is handled via the <code>ports</code> property of an application in container. Under the hood, we use a Kubernetes Service.</p>"},{"location":"adt/containers/services/#creating-a-service","title":"Creating a service","text":"<p>Let's say we want to give other Pods running on the cluster the ability to communicate with our database container. Here's the simplest way to do it:</p> <pre><code>sql-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: mysql\nports:\n- port: 3306\ninterfaces:\nKubernetes:\ncreate:\n</code></pre> <p>If you're familiar with Kubernetes, the one little snippet above generates the two manifests in the box below.</p> Kubernetes manifests <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: sql-container\nlabels:\napp.kubernetes.io/name: sql-container\napp.kubernetes.io/instance: service-ex\napp.kubernetes.io/managed-by: micado\nspec:\nselector:\napp.kubernetes.io/name: sql-container\napp.kubernetes.io/instance: service-ex\napp.kubernetes.io/managed-by: micado\napp.kubernetes.io/version: latest\nports:\n- name: 3306-tcp\nport: 3306\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: sql-container\nlabels:\napp.kubernetes.io/name: sql-container\napp.kubernetes.io/instance: service-ex\napp.kubernetes.io/managed-by: micado\napp.kubernetes.io/version: latest\nspec:\ntemplate:\nmetadata:\nlabels:\napp.kubernetes.io/name: sql-container\napp.kubernetes.io/instance: service-ex\napp.kubernetes.io/managed-by: micado\napp.kubernetes.io/version: latest\nspec:\ncontainers:\n- image: mysql\nselector:\nmatchLabels:\napp.kubernetes.io/name: sql-container\napp.kubernetes.io/instance: service-ex\napp.kubernetes.io/managed-by: micado\napp.kubernetes.io/version: latest\n</code></pre> <p>Now, any other container can access port 3306 on this container at <code>sql-container:3306</code>. You can see the hostname defaults to the name of the TOSCA node you've defined.</p> <p>If you have multiple ports to expose using the same hostname, you can simply add to the list of ports, like so:</p> <pre><code>...\nproperties:\nimage: mysql\nports:\n- port: 3306\n- port: 33062\n...\n</code></pre> <p>Other containers can reach both <code>sql-container:3306</code> and <code>sql-container:33062</code></p>"},{"location":"adt/containers/services/#protocols-and-targetport","title":"Protocols and TargetPort","text":"<p>Ports default to TCP. For UDP connections, simply:</p> <pre><code>...\nproperties:\nimage: flannel\nports:\n- port: 8285\nprotocol: UDP\n...\n</code></pre> <p>Kubernetes also supports the SCTP protocol</p> <p>Sometimes, you may want/need to expose your container on a different port than that configured by the container by default. Consider an NGINX container listening on port 80, that we intend to expose to other containers using port 8080:</p> <pre><code>webserver:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: nginx\nports:\n- port: 8080\ntargetPort: 80\n...\n</code></pre> <p>Other containers on the cluster can point at <code>webserver:8080</code> to hit <code>:80</code> inside the container</p>"},{"location":"adt/containers/services/#hostnames-and-port-names","title":"Hostnames and Port names","text":"<p>As we've seen, the name of the generated Service (also the hostname for ingress to the container) defaults to the name of the parent container. You can change this by overriding the Service metadata like so:</p> <pre><code>sql-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: mysql\nports:\n- port: 3306\nmetadata:\nname: database\n...\n</code></pre> <p>Using the above snippet, you would now reach your container at <code>database:3306</code></p> <p>It is important not to confuse this with the name of the port, which is generated automatically based on the port number and protocol (see the previously generated manifests for an example of this).</p> <p>If you have the urge to specify your own port names, you can: </p> <pre><code>...\nproperties:\nimage: mysql\nports:\n- name: myport\nport: 3306\nmetadata:\nname: database\n...\n</code></pre> <p>The port name has changed, but has no effect on our route to this container. It is still reachable at <code>database:3306</code></p>"},{"location":"adt/containers/services/#clusterips","title":"ClusterIPs","text":"<p>When we create a service, Kubernetes randomly assigns a ClusterIP to it, restricted by default to the range <code>10.96.0.0/12</code>.</p> <p>If you have a requirement for IP addressing over using hostnames, you can specify a ClusterIP like so:</p> <pre><code>sql-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: mysql\nports:\n- port: 3306\nclusterIP: 10.97.101.98\n...\n</code></pre> <p>Fun fact: you can specify a clusterIP of <code>None</code></p> <p>We can now hit this container at <code>10.97.101.98:3306</code>. It is also still available at <code>sql-container:3306</code>.</p>"},{"location":"adt/containers/services/#next-up-exposing-containers-externally","title":"Next up: Exposing containers externally","text":""},{"location":"adt/containers/sidecars/","title":"Describing Containers","text":"<ul> <li>The Basics</li> <li>Services and container communication</li> <li>Exposing containers externally</li> <li>Volumes and Configs</li> <li>Multi-Container Pods / Sidecars (this page)</li> <li>Special MiCADO/Kubernetes Types</li> </ul>"},{"location":"adt/containers/sidecars/#peas-in-a-pod","title":"Peas in a Pod","text":"<p>Sometimes its useful to have more than just one container in a Pod - say if you're following the Sidecar Pattern, or need an Init Container. Here are a few ways you can create multi-container Pods in an ADT:</p>"},{"location":"adt/containers/sidecars/#attach-sidecar-to-main","title":"Attach sidecar to main","text":"<p>First, define your main container as you normally would. It can be as complex or as simple as you like.</p> <p>Define your sidecar container as well, but do not specify any interface for it - after all, it won't be independently orchestrated by Kubernetes.</p> <p>Lastly, reference the dependent sidecar in the <code>requirements</code> section of you main container. Here's how it looks:</p> <pre><code>app-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: uowcpc/nginx:v1.2\nrequirements:\n- host: virtual-machine-one\n- container: sidecar-container\n- volume: my-volume\ninterfaces:\nKubernetes:\ncreate:\n\nsidecar-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: logstack\nargs: [\"log\", \"now\"]\n</code></pre> <p>As you can see, our sidecar container has no interface defined for it</p>"},{"location":"adt/containers/sidecars/#init-containers","title":"Init Containers","text":"<p>Init Containers run as a second container in your Pod to assert that your primary container is ready. Only when the Init Container has run to completion and <code>exit 0</code>, will your main container transition to a ready state. </p> <p>The method of attaching an Init Container is the same as the method above, though the type of the Init Container must be <code>tosca.nodes.MiCADO.Container.Application.Docker.Init</code>. </p> <p>Ensure that the Init Container is properly configured to evaluate the ready state of the main container and <code>exit 0</code> when done.</p> <pre><code>app-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: uowcpc/nginx:v1.2\nrequirements:\n- container: init-container\n...(truncated)...\n\ninit-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker.Init\nproperties:\nimage: busybox\nargs: [\"curl\", \"localhost:5000\"]\n</code></pre> <p>Make sure you specify the correct type for the Init Container, otherwise it will be included in the Pod as a regular sidecar.</p>"},{"location":"adt/containers/sidecars/#attach-two-containers-to-a-pod","title":"Attach two containers to a Pod","text":"<p>Define your containers as you normally would. You can attach volumes to both of them, but do not specify a host virtual machine, and do not specify any interface for these containers. </p> <p>Now define your empty Pod using the type <code>tosca.nodes.MiCADO.Container.Application.Pod</code>. You are welcome to define a host virtual machine requirement, and you must specify the interface so Kubernetes knows to create this Pod. Any <code>inputs</code> to the interface will be added to the manifest as normal.</p> <p>Lastly, reference the two containers you created in the <code>requirements</code> section of you main container. Like this:</p> <pre><code>app-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: uowcpc/nginx:v1.2\nrequirements:\n- volume: my-volume\n\nsidecar-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: logstack\nargs: [\"log\", \"now\"]\n\nmy-pod:\ntype: tosca.nodes.MiCADO.Container.Application.Pod\nrequirements:\n- container: app-container\n- container: sidecar-container\n- host: virtual-machine-one\ninterfaces:\nKubernetes:\ncreate:\n</code></pre> <p>The end result will be the same as in our previous example - a Pod with the two containers - but here we take a different approach.</p>"},{"location":"adt/containers/sidecars/#next-up-special-micadokubernetes-types","title":"Next up: Special MiCADO/Kubernetes Types","text":""},{"location":"adt/containers/specification/","title":"API Reference","text":"<p>This page provides a reference for the different available fields on container, volume, config and other related nodes in the ADT.</p>"},{"location":"adt/containers/specification/#containers","title":"Containers","text":""},{"location":"adt/containers/specification/#properties","title":"Properties","text":"<p>The fields under the properties section of the Kubernetes app are a collection of options specific to all iterations of Docker containers. The translator understands both Docker-Compose style naming and Kubernetes style naming, though the Kubernetes style is recommended. You can find additional information about properties at this link. These properties will be translated into Kubernetes manifests on deployment.</p> <p>Here are the fields available under properties</p> <ul> <li>name: name for the container (defaults to the TOSCA node name)</li> <li>command: override the default command line of the container (list)</li> <li>args: override the default entrypoint of container (list)</li> <li> <p>env: list of required environment variables in format:</p> </li> <li> <p>name:</p> </li> <li>value:</li> <li>valueFrom: for use with ConfigMaps, see below</li> <li>envFrom: mostly for using ConfigMaps, see below</li> <li> <p>resource:</p> </li> <li> <p>requests:</p> <ul> <li>cpu: CPU reservation, core components usually require 100m so assume   900m as a maximum</li> <li>ports: list of published ports to the host machine, you can specify these   keywords in the style of a flattened (Service, ServiceSpec and   ServicePort can all be defined at the same level - <code>see Kubernetes Service   &lt;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#service-v1-core&gt;</code>__)</li> </ul> </li> <li> <p>targetPort: the port to target (assumes port if not specified)</p> </li> <li>port: the port to publish (assumes targetPort if not specified)</li> <li>name: the name of this port in the service (generated if not specified)</li> <li>protocol: the protocol for the port (defaults to: TCP)</li> <li>nodePort: the port (30000-32767) to expose on the host     (will create a nodePort Service unless type is explicitly set below)</li> <li>type: the type of service for this port (defaults to: ClusterIP     unless nodePort is defined above)</li> <li>clusterIP: the desired (internal) IP (10.0.0.0/24) for this service     (defaults to next available)</li> <li>metadata: service metadata, giving the option to set a name for the     service. Explicit naming can be used to group different ports together     (default grouping is by type)</li> <li>hostPort: the port on the node host to expose the pod at</li> <li>containerPort: the port to target if exposing with hostPort</li> </ul> <p>Environment variables can be loaded in from configuration data in Kubernetes ConfigMaps. This can be accomplished by using envFrom: with a list of configMapRef: to load all data from a ConfigMap into environment variables as seen <code>here &lt;https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#configure-all-key-value-pairs-in-a-configmap-as-container-environment-variables&gt;</code> , or by using env: and valueFrom:  with configMapKeyRef: to load specific values into environment variables as seen <code>here &lt;https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#define-container-environment-variables-using-configmap-data&gt;</code> .</p> <p>Alternatively, ConfigMaps can be mounted as volumes as discussed <code>here &lt;https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#add-configmap-data-to-a-volume&gt;</code>__ , in the same way other volumes are attached to a container, using the requirements: notation below. Also see the examples in Specification of Configuration Data below.</p>"},{"location":"adt/containers/specification/#requirements","title":"Requirements","text":"<p>Under the requirements section you can define the virtual machine you want to host this particular app, restricting the container to run only on that VM. If you do not provide a host requirement, the container will run on any possible virtual machine. You can also attach a volume or ConfigMap to this app - the definition of volumes can be found in the next section. Requirements takes a list of map objects:</p> <ul> <li>host: name of your virtual machine as defined under node_templates</li> <li> <p>volume:</p> </li> <li> <p>node: name of your volume (or ConfigMap) as defined under     node_templates</p> </li> <li> <p>relationship: !!</p> <ul> <li>type: <code>tosca.relationships.AttachesTo</code></li> <li> <p>properties:</p> </li> <li> <p>location: path in container</p> </li> </ul> </li> <li> <p>container: name of a sidecar container defined as a   <code>tosca.nodes.MiCADO.Container.Application.Docker</code> type under   node_templates. The sidecar will share the Kubernetes Pod with   the main container (the sidecar should not be given an interface)   OR name of an init container defined as a   <code>tosca.nodes.MiCADO.Container.Application.Docker.Init</code> type   under node_templates. The Pod will enter a ready state when   the Init Container runs to completion and exits cleanly (ie. with   a zero exit code)</p> </li> </ul> <p>If a relationship is not defined for a volume the path on container will be the same as the path defined in the volume (see Specification of Volumes). If no path is defined in the volume, the path defaults to /etc/micado/volumes for a Volume or /etc/micado/configs for a ConfigMap</p>"},{"location":"adt/containers/specification/#interfaces","title":"Interfaces","text":"<p>Under the interfaces section you can define orchestrator specific options, to instruct MiCADO to use Kubernetes, we use the key Kubernetes. Fields under inputs: will be translated directly to a Kubernetes manifest so it is possible to use the full range of properties which Kubernetes offers as long as field names and syntax follow <code>the Kubernetes documentation &lt;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#deployment-v1-apps&gt;</code>__ If inputs: is omitted a set of defaults will be used to create a Deployment</p> <ul> <li> <p>create: this key tells MiCADO to create a workload (Deployment/DaemonSet/Job/Pod etc...) for this container</p> </li> <li> <p>inputs: top-level workload and workload spec options go here...     two examples, for more see <code>translator documentation &lt;https://github.com/jaydesl/TOSCAKubed/blob/master/README.md&gt;</code>__</p> <ul> <li>kind: overwrite the workload type (defaults to Deployment)</li> <li> <p>spec:</p> </li> <li> <p>strategy:</p> <ul> <li>type: Recreate (kill pods then update instead of RollingUpdate)</li> </ul> </li> </ul> </li> </ul>"},{"location":"adt/containers/specification/#networking-in-kubernetes","title":"Networking in Kubernetes","text":"<p>Kubernetes networking is inherently different to the approach taken by Docker/Swarm. This is a complex subject which is worth a read here. Since every pod gets its own IP, which any pod can by default use to communicate with any other pod, this means there is no network to explicitly define. If the ports keyword is defined in the definition above, pods can reach each other over CoreDNS via their hostname (container name).</p>"},{"location":"adt/containers/specification/#volumes","title":"Volumes","text":"<p>Volumes are defined at the same level as virtual machines and containers, and are then connected to containers using the requirements: notation discussed above in the container spec. Some examples of attaching volumes will follow.</p>"},{"location":"adt/containers/specification/#interfaces_1","title":"Interfaces","text":"<p>Under the interfaces section you should define orchestrator specific options, here we again use the key Kubernetes:</p> <ul> <li> <p>create: this key tells MiCADO to create a persistent volume and claim</p> </li> <li> <p>inputs: persistent volume specific spec options... here are two     popular examples, see <code>Kubernetes volumes &lt;https://kubernetes.io/docs/concepts/storage/volumes/&gt;</code>__ for more</p> <ul> <li> <p>spec:</p> </li> <li> <p>nfs:</p> <ul> <li>server: IP of NFS server</li> <li>path: path on NFS share</li> </ul> </li> <li> <p>hostPath:</p> <ul> <li>path: path on host</li> </ul> </li> </ul> </li> </ul>"},{"location":"adt/containers/specification/#configs","title":"Configs","text":"<p>Configuration data (a Kubernetes ConfigMap) are to be defined at the same level as virtual machines, containers and volumes and then loaded into environment variables, or mounted as volumes in the definition of containers as discussed in Specification of the Application. Some examples of using configurations will follow at the end of this section.</p>"},{"location":"adt/containers/specification/#interfaces_2","title":"Interfaces","text":"<p>Currently MiCADO only supports the definition of configuration data as Kubernetes ConfigMaps. Under the interfaces section of this type use the key Kubernetes: to instruct MiCADO to create a ConfigMap.</p> <ul> <li> <p>create: this key tells MiCADO to create a ConfigMap</p> </li> <li> <p>inputs: ConfigMap fields to be overwritten, for more detail see     <code>ConfigMap &lt;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#configmap-v1-core&gt;</code>__</p> <ul> <li>data: for UTF-8 byte values</li> <li>binaryData: for byte values outside of the UTF-8 range</li> </ul> </li> </ul>"},{"location":"adt/containers/volumes/","title":"Describing Containers","text":"<ul> <li>The Basics</li> <li>Services and container communication</li> <li>Exposing containers externally</li> <li>Volumes and Configs (this page)</li> <li>Multi-Container Pods (Sidecars)</li> <li>Special MiCADO/Kubernetes Types</li> </ul>"},{"location":"adt/containers/volumes/#volumes-and-configs","title":"Volumes and Configs","text":"<p>For persisting configurations and data across different deployments, we support Kubernetes Volumes and Kubernetes ConfigMaps. </p> <p>Volumes and ConfigMaps are described under <code>node_templates</code> of a <code>topology_template</code>, and get their own description, just like a container or virtual machine. We can then reference a volume or config under the <code>requirements</code> section of a container, in the same way we specify a host. Here's an example:</p> <pre><code>node_templates:\n\napp-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\n...\nrequirements:\n- host: worker-virtualmachine\n- volume: persistent-storage\n\nworker-virtualmachine:\ntype: tosca.nodes.MiCADO.EC2.Compute\nproperties:\n...\n\npersistent-storage:\ntype: tosca.nodes.MiCADO.Container.Volume\nproperties:\n...\n</code></pre>"},{"location":"adt/containers/volumes/#creating-common-volumes","title":"Creating Common Volumes","text":"<p>In theory, MiCADO supports any of the Kubernetes Volume types. Here are three common examples to get you started:</p>"},{"location":"adt/containers/volumes/#emptydir","title":"EmptyDir","text":"<p>This is a volatile volume for very temporary storage purposes. It will survive a Pod crash, but will be deleted when the Pod is removed.</p> <pre><code>temporary-storage:\ntype: tosca.nodes.MiCADO.Container.Volume\ninterfaces:\nKubernetes:\ncreate:\ninputs:\nspec:\nemptyDir: {}\n</code></pre> <p>That's it!</p>"},{"location":"adt/containers/volumes/#hostpath","title":"HostPath","text":"<p>These volumes are for when you want to share data to and/or from the host. In a multi-node setting this gets tricky since your Pod might jump to different nodes, with different stored data.</p> <pre><code>docker-socket:\ntype: tosca.nodes.MiCADO.Container.Volume\ninterfaces:\nKubernetes:\ncreate:\ninputs:\nspec:\nhostPath:\npath: /var/run/docker.sock\n</code></pre> <p>Here we define the path on the host</p>"},{"location":"adt/containers/volumes/#nfs","title":"NFS","text":"<p>Network File System shares are a common solution for persisting data. You can host an NFS server outside of a MiCADO deployment, and then use MiCADO to mount shares into your deployed containers.</p> <pre><code>docker-socket:\ntype: tosca.nodes.MiCADO.Container.Volume\ninterfaces:\nKubernetes:\ncreate:\ninputs:\nspec:\nnfs:\nserver: 192.168.1.1\npath: /nfs/shared\n</code></pre> <p>Note You'll need to install <code>nfs-common</code> on your virtual machines - you can see it done here</p>"},{"location":"adt/containers/volumes/#using-volumes","title":"Using Volumes","text":"<p>As we saw, we can mount volumes inside our containers by specifying them under <code>requirements</code> in the container description. By default, MiCADO creates a mount point in our container for volumes mounted in this way at <code>/mnt/volumes/name-of-volume</code>. </p> <p>Most of the time, we need more control over where our volumes are mounted, and we can accomplish that by using the following syntax:</p> <pre><code>app-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\n...\nrequirements:\n- volume:\nname: persistent-storage\nrelationship:\ntype: tosca.relationships.AttachesTo\nproperties:\nlocation: /var/www/html\n\npersistent-storage:\ntype: tosca.nodes.MiCADO.Container.Volume\nproperties:\n...\n</code></pre> <p>Define the mount point inside your container using <code>location</code></p> <p>Note You may be able to save a few lines in your ADT by specifying the <code>path</code> property when you first define your volumes. MiCADO will use <code>path</code> (if it exists) as the mount point if <code>location</code> is not defined, before generating the <code>/mnt/micado</code> path. For example:</p> <pre><code>app-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\n...\nrequirements:\n- volume: persistent-storage\n\npersistent-storage:\ntype: tosca.nodes.MiCADO.Container.Volume\nproperties:\npath: /var/www/html\n...\n</code></pre> <p>Since we've not specified <code>location</code> under <code>requirements</code>, MiCADO will set the mount point of the volume to the <code>path</code> provided in the volume definition (/var/www/html).</p>"},{"location":"adt/containers/volumes/#creating-configs","title":"Creating Configs","text":"<p>Configs are even easier than volumes. Simply pass a set of <code>key:value</code> pairs to <code>data</code> like so:</p> <pre><code>hdfs-config:\ntype: tosca.nodes.MiCADO.Container.Config\ninterfaces:\nKubernetes:\ncreate:\ninputs:\ndata:\nHDFS-SITE.XML_dfs.namenode: \"/data/namenode\"\nHDFS-SITE.XML_dfs.datanode: \"/data/datanode\"\n</code></pre> <p>For binary data, use <code>binaryData</code> under inputs</p>"},{"location":"adt/containers/volumes/#using-configs-environment","title":"Using Configs (Environment)","text":"<p>We can load all the <code>key:value</code> pairs from a created ConfigMap into environment variables like so:</p> <pre><code>app-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: hadoop\nenvFrom:\n- configMapRef:\nname: hdfs-config\ninterfaces:\nKubernetes:\ncreate:\n</code></pre> <p>Here we're using the <code>envFrom</code> property</p> <p>Alternatively we can select a single value from the ConfigMap and assign it to a new environment variable, like so:</p> <pre><code>...\nproperties:\nimage: hadoop\nenv:\n- name: HDFS_NAMENODE_DIR\nvalueFrom:\nconfigMapKeyRef:\nname: hdfs-config\nkey: HDFS-SITE.XML_dfs.namenode\n...\n</code></pre> <p>Here we're using the regular <code>env</code> property, with <code>valueFrom</code></p>"},{"location":"adt/containers/volumes/#using-configs-mounts","title":"Using Configs (Mounts)","text":"<p>Instead of using environment variables, we can mount a created ConfigMap in a container, just like we would a volume:</p> <pre><code>app-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\nimage: hadoop\nrequirements:\n- volume:\nname: hdfs-config\nrelationship:\ntype: tosca.relationships.AttachesTo\nproperties:\nlocation: /etc/hdfs/config\ninterfaces:\nKubernetes:\ncreate:\n</code></pre> <p>Pro tip: Just like with volumes, if you define the <code>path</code> property on your ConfigMap, MiCADO will use it as a default mount point (meaning you can just write <code>- volume: hdfs-config</code>)</p> <p>When a ConfigMap gets mounted into a container, Kubernetes creates a file named after each key in the ConfigMap. Each file is then populated with its matching value. This can be useful for creating configuration files in a container, for example:</p> <pre><code>nginx-config:\ntype: tosca.nodes.MiCADO.Container.Config.Kubernetes\ninterfaces:\nKubernetes:\ncreate:\ninputs:\ndata:\nnginx.conf: |\nevents {}\n\nhttp {\nserver {\nlisten 8000;\n}\n}\n</code></pre> <p>When this ConfigMap gets mounted, the file <code>nginx.conf</code> will be created at the mount point, populated with the given value</p>"},{"location":"adt/containers/volumes/#next-up-multi-container-pods-sidecars","title":"Next up: Multi-Container Pods (Sidecars)","text":""},{"location":"adt/policies/","title":"Policies","text":"<p>ADT policies drive the runtime behaviour of the nodes and containers that make up your application. From here, you can enable monitoring, set scaling rules, pick network policies and even manage application-level secrets.</p>"},{"location":"adt/policies/monitoring/","title":"Monitoring Policies","text":"<p>Metric collection is disabled by default. The node exporter for  can be enabled through the monitoring policy below. If the policy is omitted, or if one property is left undefined, then the relevant metric collection will be disabled.</p> <pre><code>policies:\n- monitoring:\ntype: tosca.policies.Monitoring.MiCADO\nproperties:\nenable_container_metrics: true\nenable_node_metrics: true\n</code></pre>"},{"location":"adt/policies/networking/","title":"Network policies","text":"<p>There are six types of MiCADO network security policy.</p>"},{"location":"adt/policies/networking/#passthrough","title":"Passthrough","text":"<p><code>tosca.policies.Security.MiCADO.Network.Passthrough</code></p> <p>Pass through network policy. Specifies no additional filtering, no application-level firewall on the nodes.</p>"},{"location":"adt/policies/networking/#l7proxy","title":"L7Proxy","text":"<p><code>tosca.policies.Security.MiCADO.Network.L7Proxy</code> </p> <p>Apply application-level firewall; can provide TLS control. No protocol enforcement.</p> Properties <pre><code>    properties:\nencryption:\ntype: boolean\ndescription: Specifies if encryption should be used\nrequired: true\nencryption_key:\ntype: string\ndescription: The key file for TLS encryption as unencrypted .PEM\nrequired: false\nencryption_cert:\ntype: string\ndescription: The cert file for TLS encryption as .PEM\nrequired: false\nencryption_offload:\ntype: string\ndescription: Controls whether connection should be re-encrypted server side\nrequired: false\nencryption_cipher:\ntype: string\ndescription: Specifies allowed ciphers client side during TLS handshake\nrequired: false\n</code></pre>"},{"location":"adt/policies/networking/#smtp-proxy","title":"SMTP Proxy","text":"<p><code>tosca.policies.Security.MiCADO.Network.SmtpProxy</code></p> <p>Enforce SMTP protocol, can provide TLS control.</p> Properties <pre><code>    properties:\nrelay_check:\ntype: boolean\ndescription: Toggle relay checking\nrequired: true\npermit_percent_hack:\ntype: boolean\ndescription: Allow the % symbol in the local part of an email address\nrequired: false\nerror_soft:\ntype: boolean\ndescription: Return a soft error when recipient filter does not match\nrequired: false\nrelay_domains:\ntype: list\ndescription: Domain mails are accepted for use postfix style lists\nrequired: false\npermit_exclamation_mark:\ntype: boolean\ndescription: Allow the ! symbol in the local part of an email address\nrequired: false\nrelay_domains_matcher_whitelist:\ntype: list\ndescription: Domains mails accepted based on list of regex (precedence)\nrequired: false\nrelay_domains_matcher_blacklist:\ntype: list\ndescription: Domain mails rejected based on list of regular expressions\nrequired: false\nsender_matcher_whitelist:\ntype: list\ndescription: Sender addresses accepted based on list of regex (precedence)\nrequired: false\nsender_matcher_blacklist:\ntype: list\ndescription: Sender addresses rejected based on list of regex\nrequired: false\nrecipient_matcher_whitelist:\ntype: list\ndescription: Recipient addresses accepted based on list of regex (precedence)\nrequired: false\nrecipient_matcher_blacklist:\ntype: list\ndescription: Recipient addresses rejected based on list of regex\nrequired: false\nautodetect_domain_from:\ntype: string\ndescription: Let Zorp autodetect firewall domain name and write to received line\nconstraints:\n- valid_values: [\"mailname\", \"fqdn\"]\nrequired: false\nappend_domain:\ntype: string\ndescription: Domain to append to email addresses which do not specify a domain\nrequired: false\npermit_omission_of_angle_brackets:\ntype: boolean\ndescription: Permit MAIL From and RCPT To params without normally required brackets\nrequired: false\ninterval_transfer_noop:\ntype: integer\ndescription: Interval between two NOOP commands sent to server while waiting for stack proxy results\nrequired: false\nresolve_host:\ntype: boolean\ndescription: Resolve client host from IP address and write to received line\nrequired: false\npermit_long_responses:\ntype: boolean\ndescription: Permit overly long responses as some MTAs include variable parts in responses\nrequired: false\nmax_auth_request_length:\ntype: integer\ndescription: Maximum allowed length of a request during SASL style authentication\nrequired: false\nmax_response_length:\ntype: integer\ndescription: Maximum allowed line length of server response\nrequired: false\nunconnected_response_code:\ntype: integer\ndescription: Error code sent to client if connecting to server fails\nrequired: false\nadd_received_header:\ntype: boolean\ndescription: Add a received header into the email messages transferred by proxy\nrequired: false\ndomain_name:\ntype: string\ndescription: Fix a domain name into added receive line. add_received_header must be true\nrequired: false\ntls_passthrough:\ntype: boolean\ndescription: Change to passthrough mode\nrequired: false\nextensions:\ntype: list\ndescription: Allowed ESMTP extensions, indexed by extension verb\nrequired: false\nrequire_crlf:\ntype: boolean\ndescription: Specify whether proxy should enforce valid CRLF line terminations\nrequired: false\ntimeout:\ntype: integer\ndescription: Timeout in ms - if no packet arrives, connection is dropped\nrequired: false\nmax_request_length:\ntype: integer\ndescription: Maximum allowed line length of client requests\nrequired: false\npermit_unknown_command:\ntype: boolean\ndescription: Enable unknown commands\nrequired: false\n</code></pre>"},{"location":"adt/policies/networking/#http-proxy","title":"HTTP Proxy","text":"<p><code>tosca.policies.Security.MiCADO.Network.HttpProxy</code></p> <p>Enforce HTTP protocol, can provide TLS control.</p> Properties <pre><code>properties:\nmax_keepalive_requests:\ntype: integer\ndescription: Max number of requests allowed in a single session\nrequired: false\npermit_proxy_requests:\ntype: boolean\ndescription: Allow proxy type requests in transparent mode\nrequired: false\nreset_on_close:\ntype: boolean\ndescription: If connection is terminated without a proxy generated error, send an RST instead of a normal close\nrequired: false\npermit_unicode_url:\ntype: boolean\ndescription: Allow unicode characters in URLs encoded as u'\nrequired: false\npermit_server_requests:\ntype: boolean\ndescription: Allow server type requests in non transparent mode\nrequired: false\nmax_hostname_length:\ntype: integer\ndescription: Maximum allowed length of hostname field in URLs\nrequired: false\nparent_proxy:\ntype: string\ndescription: Address or hostname of parent proxy to be connected\nrequired: false\npermit_ftp_over_http:\ntype: boolean\ndescription: Allow processing FTP URLs in non transparent mode\nrequired: false\nparent_proxy_port:\ntype: integer\ndescription: Port of parent proxy to be connected\nrequired: false\npermit_http09_responses:\ntype: boolean\ndescription: Allow server responses to use limited HTTP 0 9 protocol\nrequired: false\nrewrite_host_header:\ntype: boolean\ndescription: Rewrite host header in requests when URL redirection occurs\nrequired: false\nmax_line_length:\ntype: integer\ndescription: Maximum allowed length of lines in requests and responses\nrequired: false\nmax_chunk_length:\ntype: integer\ndescription: Maximum allowed length of a single chunk when using chunked transer encoding\nrequired: false\nstrict_header_checking_action:\ntype: string\ndescription: Specify Zorp action if non rfc or unknown header in communication\nconstraints:\n- valid_values: [\"accept\", \"drop\", \"abort\"]\nrequired: false\nnon_transparent_ports:\ntype: list\ndescription: List of ports that non transparent requests may use\nrequired: false\nstrict_header_checking:\ntype: boolean\ndescription: Require RFC conformant HTTP headers\nrequired: false\nmax_auth_time:\ntype: integer\ndescription: Force new auth request from client browser after time in seconds\nrequired: false\nmax_url_length:\ntype: integer\ndescription: Maximum allowed length of URL in a request\nrequired: false\ntimeout_request:\ntype: integer\ndescription: Time to wait for a request to arrive from client\nrequired: false\nrerequest_attempts:\ntype: integer\ndescription: Control number of attempts proxy takes to send request to server\nrequired: false\nerror_status:\ntype: integer\ndescription: On error, Zorp uses this as status code of HTTP response\nrequired: false\nkeep_persistent:\ntype: boolean\ndescription: Try to keep connection to client persistent, even if unsupported\nrequired: false\nerror_files_directory:\ntype: string\ndescription: Location of HTTP error messages\nrequired: false\nmax_header_lines:\ntype: integer\ndescription: Maximum number of eader lines allowed in requests and responses\nrequired: false\nuse_canonicalized_urls:\ntype: boolean\ndescription: Enable canonicalization - converts URLs to canonical form\nrequired: false\nmax_body_length:\ntype: integer\ndescription: Maximum allowed length of HTTP request or response body\nrequired: false\nrequire_host_header:\ntype: boolean\ndescription: Require presence of host header\nrequired: false\nbuffer_size:\ntype: integer\ndescription: Size of I O buffer used to transfer entity bodies\nrequired: false\npermitted_responses:\ntype: list\ndescription: Normative policy hash for HTTP responses indexed by HTTP method and response code\nentry_schema:\ndescription: dictionary (string/int)\ntype: map\nrequired: false\ntransparent_mode:\ntype: boolean\ndescription: Enable transparent mode for the proxy\nrequired: false\npermit_null_response:\ntype: boolean\ndescription: Permit RFC incompliant responses with headers not terminated by CRLF, and not containing entity body\nrequired: false\nlanguage:\ntype: string\ndescription: Specify language of HTTP error pages displayed to client\nrequired: false\ndefault: English\nerror_silent:\ntype: boolean\ndescription: Turns off verbose error reporting to HTTP client, making firewall fingerprinting more difficult\nrequired: false\npermitted_requests:\ntype: list\ndescription: List of permitted HTTP methods indexed by verb\nrequired: false\nuse_default_port_in_transparent_mode:\ntype: boolean\ndescription: Enable use of default port in transparent mode\nrequired: false\ntimeout_response:\ntype: integer\ndescription: Time to wait for the HTTP status line to arrive from the server\nrequired: false\npermit_invalid_hex_escape:\ntype: boolean\ndescription: Allow invalid hexadecimal escaping in URLs\nrequired: false\nauth_cache_time:\ntype: integer\ndescription: Caching authentication information time in seconds\nrequired: false\ntimeout:\ntype: integer\ndescription: General I O timeout in ms\nrequired: false\ndefault_port:\ntype: integer\ndescription: Used in non transparent mode when URL does not contain a port number\nrequired: false\ndefault: 80\n</code></pre>"},{"location":"adt/policies/networking/#http-uri-filter-proxy","title":"HTTP URI Filter Proxy","text":"<p><code>tosca.policies.Security.MiCADO.Network.HttpURIFilterProxy</code></p> <p>Enforce HTTP protocol with regex URL filtering capabilities.</p> Properties <pre><code>    properties:\nmatcher_whitelist:\ntype: list\ndescription: List of regex determining permitted access to a URL (precedence)\nrequired: true\nmatcher_blacklist:\ntype: list\ndescription: List of regex determining prohibited access to a URL\nrequired: true\n</code></pre>"},{"location":"adt/policies/networking/#http-webdav-proxy","title":"HTTP WebDAV Proxy","text":"<p><code>tosca.policies.Security.MiCADO.Network.HttpWebdavProxy</code></p> <p>Enforce HTTP protocol with request methods for WebDAV. This proxy has no additional properties.</p>"},{"location":"adt/policies/scaling/","title":"Scaling Policies","text":""},{"location":"adt/policies/scaling/#basic-scaling","title":"Basic scaling","text":"<p>To utilise the autoscaling functionality of MiCADO, scaling policies can be defined on virtual machine and on the application level. Scaling policies can be listed under the policies section. Each scalability subsection must have the type set to the value of <code>tosca.policies.Scaling.MiCADO</code> and must be linked to a node defined under node_template. The link can be implemented by specifying the name of the node under the targets subsection. You can attach different policies to different containers or virtual machines, though a new policy should exist for each. The details of the scaling policy can be defined under the properties subsection. The structure of the policies section can be seen below.</p> <pre><code>  topology_template:\nnode_templates:\nYOUR-VIRTUAL-MACHINE:\ntype: tosca.nodes.MiCADO.&lt;CLOUD_API_TYPE&gt;.Compute\n...\nYOUR-OTHER-VIRTUAL-MACHINE:\ntype: tosca.nodes.MiCADO.&lt;CLOUD_API_TYPE&gt;.Compute\n...\nYOUR-KUBERNETES-APP:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\n...\nYOUR-OTHER-KUBERNETES-APP:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\n...\n\npolicies:\n- scalability:\ntype: tosca.policies.Scaling.MiCADO\ntargets: [ YOUR-VIRTUAL-MACHINE ]\nproperties:\n...\n- scalability:\ntype: tosca.policies.Scaling.MiCADO\ntargets: [ YOUR-OTHER-VIRTUAL-MACHINE ]\nproperties:\n...\n- scalability:\ntype: tosca.policies.Scaling.MiCADO\ntargets: [ YOUR-KUBERNETES-APP ]\nproperties:\n...\n- scalability:\ntype: tosca.policies.Scaling.MiCADO\ntargets: [ YOUR-OTHER-KUBERNETES-APP ]\nproperties:\n...\n</code></pre> <p>The scaling policies are evaluated periodically. In every turn, the virtual machine level scaling policies are evaluated, followed by the evaluation of each scaling policies belonging to kubernetes-deployed applications.</p>"},{"location":"adt/policies/scaling/#properties","title":"Properties","text":"<p>The properties subsection defines the scaling policy itself. For monitoring purposes, MiCADO integrates the Prometheus monitoring tool with two built-in exporters on each worker node: Node exporter (to collect data on nodes) and CAdvisor (to collect data on containers). Based on Prometheus, any monitored information can be extracted using the Prometheus query language and the returned value can be associated to a user-defined variable. Once variables are updated, scaling rule is evaluated. Scaling rule is specified by (a short) Python code. The code can refer to/use the variables. The structure of the scaling policy can be seen below.</p> <pre><code>  - scalability:\n...\nproperties:\nsources:\n- 'myprometheus.exporter.ip.address:portnumber'\nconstants:\nLOWER_THRESHOLD: 50\nUPPER_THRESHOLD: 90\nMYCONST: 'any string'\nqueries:\nTHELOAD: 'Prometheus query expression returning a number'\nMYLISTOFSTRING: ['Prometheus query returning a list of strings as tags','tagname as filter']\nMYEXPR: 'something refering to {{MYCONST}}'\nalerts:\n- alert: myalert\nexpr: 'Prometheus expression for an event important for scaling'\nfor: 1m\nmin_instances: 1\nmax_instances: 5\nscaling_rule: |\nif myalert:\nm_node_count=5\nif THELOAD&gt;UPPER_THRESHOLD:\nm_node_count+=1\nif THELOAD&lt;LOWER_THRESHOLD:\nm_node_count-=1\n</code></pre> <p>The subsections have the following roles:</p> <ul> <li>sources supports the dynamic attachment of an external exporter by specifying a list endpoints of exporters (see example above). Each item found under this subsection is configured under Prometheus to start collecting the information provided/exported by the exporters. Once done, the values of the parameters provided by the exporters become available. MiCADO supports Kubernetes service discovery to define such a source, simply pass the name of the app as defined in TOSCA and do not specify any port number</li> <li>constants subsection is used to predefined fixed parameters. Values associated to the parameters can be referred by the scaling rule as variable (see <code>LOWER_THRESHOLD</code> above) or in any other sections referred as Jinja2 variable (see <code>MYEXPR</code> above).</li> <li>queries contains the list of Prometheus query expressions to be executed and their variable name associated (see <code>THELOAD</code> or <code>MYLISTOFSTRING</code> above)</li> <li>alerts subsection enables the utilization of the alerting system of Prometheus. Each alert defined here is registered under Prometheus and fired alerts are represented with a variable of their name set to True during the evaluation of the scaling rule (see <code>myalert</code> above).</li> <li>min_instances keyword specifies the lowest number of instances valid for the node.</li> <li>max_instances keyword specifies the highest number of instances valid for the node.</li> <li> <p>scaling_rule specifies Python code to be evaluated periodically to decide on the number of instances. The Python expression must be formalized with the following conditions:</p> </li> <li> <p>Each constant defined under the \u2018constants\u2019 section can be referred; its value is the one defined by the user.</p> </li> <li>Each variable defined under the \u2018queries\u2019 section can be referred; its value is the result returned by Prometheus in response to the query string.</li> <li>Each alert name defined under the \u2018alerts\u2019 section can be referred, its value is a logical True in case the alert is firing, False otherwise</li> <li>Expression must follow the syntax of the Python language</li> <li>Expression can be multiline</li> <li> <p>The following predefined variables can be referred; their values are defined and updated before the evaluation of the scaling rule</p> <ul> <li>m_nodes: python list of nodes belonging to the kubernetes cluster</li> <li>m_node_count: the target number of nodes</li> <li>m_nodes_todrop: the ids or ip addresses of the nodes to be dropped in case of downscaling NOTE MiCADO-Terraform supports private IPs on Azure or AWS EC2 only</li> <li>m_container_count: the target number of containers for the service the evaluation belongs to</li> <li>m_time_since_node_count_changed: time in seconds elapsed since the number of nodes changed</li> </ul> </li> <li> <p>In a scaling rule belonging to the virtual machine, the name of the variable to be updated is <code>m_node_count</code>; as an effect the number stored in this variable will be set as target instance number for the virtual machines.</p> </li> <li>In a scaling rule belonging to the virtual machine, the name of the variable to be updated is <code>m_nodes_todrop</code>;the variable must be filled with list of ids or ip addresses and as an effect the valid nodes will be dropped. The variable <code>m_node_count</code> should not be modified in case of node dropping, MiCADO will update it automatically.</li> <li>In a scaling rule belonging to a kubernetes deployment, the name of the variable to be set is <code>m_container_count</code>; as an effect the number stored in this variable will be set as target instance number for the kubernetes service.</li> </ul> <p>For debugging purposes, the following support is provided:</p> <ul> <li> <p><code>m_dryrun</code> can be specified in the constant as list of components towards which the communication is disabled. It has the following syntax: m_dryrun: [\"prometheus\",\"occopus\",\"k8s\",\"optimizer\"] Use this feature with caution!</p> </li> <li> <p>the standard output of the python code defined by the user under the scaling rule section is collected in a separate log file stored under the policy keeper log directory. It can also be used for debugging purposes.</p> </li> </ul> <p>For further examples, inspect the scaling policies of the demo examples detailed in the next section.</p>"},{"location":"adt/policies/scaling/#optimiser-based-scaling","title":"Optimiser-based scaling","text":"<p>For implementing more advanced scaling policies, it is possible to utilize the built-in Optimiser in MiCADO. The role of the Optimiser is to support decision making in calculating the number of worker nodes (virtual machines) i.e. to scale the nodes to the optimal level. Optimiser is implemented using machine learning algorithm aiming to learn the relation between various metrics and the effect of scaling events. Based on this learning, the Optimiser is able to calculate and advise on the necessary number of virtual machines.</p> <p>Current limitations   - only web based applications are supported   - only one of the node sets can be supported   - no container scaling is supported</p> <p>Optimiser can be utilised based on the following principles   - User specifies a so-called target metric with its associated minimum and maximum thresholds. The target metric is a monitored Prometheus expression for which the value is tried to be kept between the two thresholds by the Optimiser with scaling advices.   - User specifies several so-called input metrics which represent the state of the system correlating to the target variable   - User specifies several initial settings (see later) for the Optimiser   - User submits the application activating the Optimiser through the ADT   - Optimiser starts with the 'training' phase during which the correlations are learned. During the training phase artificial load must be generated for the web application and scaling activities must be performed (including extreme values) in order to present all possible situations for the Optimiser. During the phase, Optimiser continuously monitors the input/target metrics and learns the correlations.   - When correlations are learnt, Optimiser turns to 'production' phase during which advice can be requested from the Optimiser. During this phase, Optimiser returns advice on request, where the advice contains the number of virtual machines (nodes) to be scaled to. During the production phase, the Optimiser continues its learning activity to adapt to the new situations.</p> <p>Activation of the Optimiser   Optimiser must be enabled at deployment time. By default it is disabled. Once it is enabled and deployed, it can be driven through the scaling policy in subsections \"constants\" and \"queries\". Each parameter relating to the Optimiser must start with the \"m_opt_\" string. In case no variable name with this prefix is found in any sections, Optimiser is not activated.</p> <p>Initial settings for the Optimiser   Parameters for initial settings are defined under the \"constants\" section and their name must start with the <code>m_opt_init_</code> prefix. These parameters are as follows:</p> <ul> <li>m_opt_init_knowledge_base is a parameter which specifies the way how the knowledge base must be built under the Optimiser. When defined as \"build_new\", Optimiser empties its knowledge base and starts building a new knowledge i.e. starts learning the correlations. When using the \"use_existing\" value, the knowledge is kept and continued building further. Default is \"use_existing\".</li> <li>m_opt_init_training_samples_required defines how many sample of the metrics must be collected by the Optimiser before start learning the correlations. Default is 300.</li> <li>m_opt_init_max_upscale_delta specifies the maximum change in number of node for an upscaling advice. Default is 6.</li> <li>m_opt_init_max_downscale_delta specifies the maximum change in number of node for a downscaling advice. Default is 6.</li> <li>m_opt_init_advice_freeze_interval specifies how many seconds must elapse before the Optimiser advises a different number of node. Can be used to mitigate the frequency of scaling. Defaults to 0.</li> </ul> <p>Definition of input metrics for the Optimizer   Input metrics must be specified for the Optimiser under the \"queries\" subsection to perform the training i.e. learning the correlations. Each parameter must start with the \"m_opt_input_\" prefix, e.g. m_opt_input_CPU. The following two pieces of variable must be specified for the web application:</p> <ul> <li>m_opt_input_AVG_RR should specify the average request rate of the web server.</li> <li>m_opt_input_SUM_RR should specify the summary of request rate of the web server.</li> </ul> <p>Definition of the target metric for the Optimizer   Target metric is a continuously monitored parameter that must be kept between thresholds. To specify it, together with the thresholds, \"m_opt_target_\" prefix must be used. These three parameter must be defined under the \"queries\" sections. They are as follows:</p> <ul> <li>m_opt_target_query_MYTARGET specifies the prometheus query for the target metric called MYTARGET.</li> <li>m_opt_target_minth_MYTARGET specifies the value above which the target metric must be kept.</li> <li>m_opt_target_maxth_MYTARGET specifies the value below which the target metric must be kept.</li> </ul> <p>Requesting scaling advice from the Optimizer   In order to receive a scaling advice from the Optimiser, the method m_opt_advice() must be invoked in the scaling_rule section of the node.</p> <p>IMPORTANT! Minimum and maximum one node must contain this method invocation in its scaling_rule section for proper operation!</p> <p>The m_opt_advice() method returns a python dictionary containing the following fields:</p> <ul> <li>valid stores True/False value indicating whether the advise can be considered or not.</li> <li>phase indicates whether the Optimiser is in \"training\" or \"production\" phase.</li> <li>vm_number represents the advise for the target number of nodes to scale to.</li> <li>reliability represents the goodness of the advice with a number between 0 and 100. The bigger the number is the better/more reliable the advice is.</li> <li>error_msg contains the error occured in the Optimiser. Filled when valid is False.</li> </ul>"},{"location":"adt/policies/secrets/","title":"Secrets policy","text":"<p>There is a way to define application-level secrets in the MiCADO application description. These secrets are managed by Security Policy Manager and stored and distributed as a single secret called micado.appsecret by Kubernetes.</p> <p>See an example below for creating a secret using policies, and assigning it to an environment variable (ENV_SALT in the example) in a container:</p> <pre><code>topology_template:\nnode_templates:\nmy-app-container:\ntype: tosca.nodes.MiCADO.Container.Application.Docker\nproperties:\n...\nenv:\n- name: ENV_SALT\nvalueFrom:\nsecretKeyRef:\nname: micado.appsecret\nkey: salt_value\n\npolicies:\n- secret:\ntype: tosca.policies.Security.MiCADO.Secret.KubernetesSecretDistribution\nproperties:\ntext_secrets:\nsalt_value: \"123456qwerty\"\n</code></pre>"},{"location":"adt/vms/","title":"Virtual Machines","text":"<p>The collection of containers specified in the previous section is orchestrated by Kubernetes.  This section introduces how the nodes that make up the Kubernetes cluster will be configured.</p> <p>During operation MiCADO will instantiate as many virtual machines with the parameters defined here as required, scaling as required. MiCADO currently supports seven different cloud interfaces:</p> <ul> <li>OpenStack Nova</li> <li>EC2</li> <li>CloudBroker</li> <li>CloudSigma</li> <li>Azure</li> <li>Google Cloud</li> <li>Oracle Cloud</li> </ul> <p>MiCADO supports multiple sets of nodes, and specific containers can be configured to run only on specific nodes. Multi-cloud support has not been fully tested across all combinations of clouds, but the underlying K3s provides excellent support for this.</p> <p>Warning</p> <p>As with containers, underscores are not permitted in virtual machine names. Names should begin and end with an alphanumeric.</p> <p>The following ports and protocols should be enabled on nodes acting as MiCADO workers, replacing <code>application dependent</code> with ports your application needs exposed on that host.</p> Protocol Port(s) Service * * application dependent UDP 51820 wireguard (Pod-Pod)"},{"location":"adt/vms/#overview","title":"Overview","text":"<p>Here is how a virtual machine description might appear inside an ADT.</p> <pre><code>wmin-openstack-worker:\ntype: tosca.nodes.MiCADO.Nova.Compute\nproperties:\nflavor_id: t2.large\n...\ncontext:\ncloud_config: |\nruncmd:\n- curl http://example.com || echo \"curl failed\" &gt; status.log\ncapabilities:\nhost:\nproperties:\nnum_cpus: 2\nmem_size: 4 GB\nos:\nproperties:\ndistribution: ubuntu\nversion: 18.04\ninterfaces:\nOccopus:\ncreate:\ninputs:\nendpoint: https://wmin-cloud.ac.uk/api/v3\n</code></pre>"},{"location":"adt/vms/#properties","title":"Properties","text":"<p>The properties section is REQUIRED and contains the necessary properties to provision the virtual machine and vary from cloud to cloud. Properties for each cloud will vary and are detailed in a separate section for each supported cloud.</p>"},{"location":"adt/vms/#cloud-contextualisation","title":"Cloud Contextualisation","text":"<p>It is possible to provide custom configuration of the deployed nodes via   cloud-init scripts.   MiCADO relies on a cloud-init config to join nodes to the   cluster, so it is recommended to only add to the default config, except   for certain cases.</p> <p>The context key is supported by all the cloud compute node definitions   below. New cloud-init configurations should be defined in cloud_config   and one of append, insert or overwrite can be set to   change the behaviour of how the default cloud-init config is affected.</p> <ul> <li>Setting append to true will add the newly defined configurations     to the end of the default cloud-init config. This is the default.</li> <li>Setting insert to true will add the newly defined configurations     to the start of the default cloud-init config, before the MiCADO Worker     is fully initialised</li> <li>Setting overwrite to true will overwrite the default cloud-init config.</li> </ul>"},{"location":"adt/vms/#capabilities-optional","title":"Capabilities (Optional)","text":"<p>The capabilities sections for all virtual machine definitions that follow are identical and are ENTIRELY OPTIONAL. They are ommited in the cloud-specific examples below. They are filled with the following metadata to support human readability:</p> <ul> <li>num_cpus under host is an integer specifying number of CPUs for   the instance type</li> <li>mem_size under host is a readable string with unit specifying RAM of   the instance type</li> <li>type under os is a readable string specifying the operating system   type of the image</li> <li>distribution under os is a readable string specifying the OS distro   of the image</li> <li>version under os is a readable string specifying the OS version of   the image</li> </ul>"},{"location":"adt/vms/#interfaces","title":"Interfaces","text":"<p>The interfaces section of all virtual machine definitions that follow are REQUIRED, and allow you to provide orchestrator specific inputs, in the examples we use either Occopus or Terraform based on suitability.</p> <ul> <li> <p>create: this key tells MiCADO to create the VM using Occopus/Terraform</p> </li> <li> <p>inputs: Extra settings to pass to Occopus or Terraform</p> <ul> <li>endpoint: the endpoint API of the cloud (always required for   Occopus, sometimes required for Terraform)</li> </ul> </li> </ul>"},{"location":"adt/vms/#pre-defined-types","title":"Pre-defined Types","text":"<p>Through abstraction, it is possible to reference a pre-defined type and simplify the description of a virtual machine.</p> <p>Example</p> <p>Here we use the <code>.Occo.small</code> extension of the <code>CloudSigma.Compute</code> parent node. Notice how we don't specify the VM size, and completely omit the interfaces section.</p> <pre><code>wmin-cloudsigma-worker:\ntype: tosca.nodes.MiCADO.CloudSigma.Compute.Occo.small\nproperties:\nvnc_password: secret\nlibdrive_id: 87ce928e-e0bc-4cab-9502-514e523783e3\npublic_key_id: d7c0f1ee-40df-4029-8d95-ec35b34dae1e\nnics:\n- firewall_policy: fd97e326-83c8-44d8-90f7-0a19110f3c9d\nip_v4_conf:\nconf: dhcp\n</code></pre> <p>Currently MiCADO supports these additional types for CloudSigma, but there is no limit to how many can be authored!</p> <ul> <li>tosca.nodes.MiCADO.EC2.Compute.Terra -   Orchestrates with Terraform on eu-west-2, overwrite region_name   under properties to change region</li> <li>tosca.nodes.MiCADO.CloudSigma.Compute.Occo -   Automatically orchestrates on Zurich with Occopus. There is no need to   define further fields under interfaces: but Zurich can be changed   by overwriting endpoint under properties:</li> <li>tosca.nodes.MiCADO.CloudSigma.Compute.Occo.small -   As above but creates a 2GHz/2GB node by default</li> <li>tosca.nodes.MiCADO.CloudSigma.Compute.Occo.big -   As above but creates a 4GHz/4GB node by default</li> <li>tosca.nodes.MiCADO.CloudSigma.Compute.Occo.small.NFS -   As small above but installs NFS dependencies by default</li> </ul>"},{"location":"adt/vms/azure/","title":"Microsoft Azure","text":""},{"location":"adt/vms/azure/#overview","title":"Overview","text":"<p>To instantiate MiCADO workers on a cloud through Azure interface, please use the template below. Currently, only Terraform has support for Azure, so Terraform must be enabled, and the interface must be set to Terraform as in the example below.</p> <p>MiCADO supports Windows VM provisioning in Azure. To force a Windows VM, simply DO NOT pass the public_key property and set the image to a desired WindowsServer Sku (2016-Datacenter). Refer to this Sku list</p> <pre><code>YOUR-VIRTUAL-MACHINE:\ntype: tosca.nodes.MiCADO.Azure.Compute\nproperties:\nresource_group: ADD_YOUR_RG_HERE (e.g. my-test)\nvirtual_network: ADD_YOUR_VNET_HERE (e.g. my-test-vnet)\nsubnet: ADD_YOUR_SUBNET_HERE (e.g. default)\nnetwork_security_group: ADD_YOUR_NSG_HERE (e.g. my-test-nsg)\nsize: ADD_YOUR_ID_HERE (e.g. Standard_B1ms)\nimage: ADD_YOUR_IMAGE_HERE (e.g. 18.04.0-LTS or 2016-Datacenter)\npublic_key: ADD_YOUR_MINIMUM_2048_KEY_HERE (e.g. ssh-rsa ASHFF...)\npublic_ip: [OPTIONAL] BOOLEAN_ENABLE_PUBLIC_IP (e.g. true)\n\ninterfaces:\nTerraform:\ncreate:\n</code></pre>"},{"location":"adt/vms/azure/#properties","title":"Properties","text":"<p>Under the properties section of a Azure virtual machine definition these inputs are available.:</p> <ul> <li>resource_group specifies the name of the resource group in which   the VM should exist.</li> <li>virtual_network specifies the virtual network associated with the VM.</li> <li>subnet specifies the subnet associated with the VM.</li> <li>network_security_group specifies the security settings for the VM.</li> <li>vm_size specifies the size of the VM.</li> <li>image specifies the name of the image.</li> <li>public_ip [OPTIONAL] Associate a public IP with the VM.</li> <li>key_data The public SSH key (minimum 2048-bit) to be associated with   the instance.   Defining this property forces creation of a Linux VM. If it is not defined, a Windows VM will be created</li> </ul>"},{"location":"adt/vms/azure/#interfaces","title":"Interfaces","text":"<p>Under the interfaces section of a Azure virtual machine definition no specific inputs are required, but Terraform: create: should be present</p>"},{"location":"adt/vms/azure/#authentication","title":"Authentication","text":"<p>Authentication in Azure is supported by MiCADO in two ways:</p> <ul> <li> <p>The first is by setting up a   Service Principal   and providing the required fields in during   cloud credential configuration.</p> </li> <li> <p>The other option is by enabling a   System-Assigned Managed Identity   on the MiCADO Control Plane and then   modify access control   of the current subscription to assign the role of Contributor to   the MiCADO Control Plane</p> </li> </ul>"},{"location":"adt/vms/cloudbroker/","title":"CloudBroker","text":""},{"location":"adt/vms/cloudbroker/#overview","title":"Overview","text":"<p>To instantiate MiCADO workers on CloudBroker, please use the template below. MiCADO requires deployment_id and instance_type_id to instantiate a VM on CloudBroker.</p> <p>Currently, only Occopus has support for CloudBroker, so Occopus must be enabled and the interface must be set to Occopus as in the example below.</p> <pre><code>YOUR-VIRTUAL-MACHINE:\ntype: tosca.nodes.MiCADO.CloudBroker.Compute\nproperties:\ndeployment_id: ADD_YOUR_ID_HERE (e.g. e7491688-599d-4344-95ef-aff79a60890e)\ninstance_type_id: ADD_YOUR_ID_HERE (e.g. 9b2028be-9287-4bf6-bbfe-bcbc92f065c0)\nkey_pair_id: ADD_YOUR_ID_HERE (e.g. d865f75f-d32b-4444-9fbb-3332bcedeb75)\nopened_port: ADD_YOUR_PORTS_HERE (e.g. '22,2377,7946,8300,8301,8302,8500,8600,9100,9200,4789')\n\ninterfaces:\nOccopus:\ncreate:\ninputs:\nendpoint: ADD_YOUR_ENDPOINT (e.g https://cola-prototype.cloudbroker.com )\n</code></pre>"},{"location":"adt/vms/cloudbroker/#properties","title":"Properties","text":"<p>Under the properties section of a CloudBroker virtual machine definition these inputs are available.:</p> <ul> <li>deployment_id is the id of a preregistered deployment in CloudBroker   referring to a cloud, image, region, etc. Make sure the image contains a   base OS (preferably Ubuntu) installation with cloud-init support! The id is   the UUID of the deployment which can be seen in the address bar of your   browser when inspecting the details of the deployment.</li> <li>instance_type_id is the id of a preregistered instance type in   CloudBroker referring to the capacity of the virtual machine to be deployed.   The id is the UUID of the instance type which can be seen in the address bar   of your browser when inspecting the details of the instance type.</li> <li>key_pair_id is the id of a preregistered ssh public key in CloudBroker   which will be deployed on the virtual machine. The id is the UUID of the key   pair which can be seen in the address bar of your browser when inspecting the   details of the key pair.</li> <li>opened_port is one or more ports to be opened to the world. This is a   string containing numbers separated by a comma.</li> </ul>"},{"location":"adt/vms/cloudbroker/#interfaces","title":"Interfaces","text":"<p>Under the interfaces section of a CloudBroker virtual machine definition, remember to pass the endpoint of the CloudBroker deployment you are using.</p>"},{"location":"adt/vms/cloudsigma/","title":"CloudSigma","text":"<p>To instantiate MiCADO workers on CloudSigma, please use the template below. MiCADO requires num_cpus, mem_size, vnc_password, libdrive_id, public_key_id and firewall_policy to instantiate VM on CloudSigma.</p> <p>Currently, only Occopus has support for CloudSigma, so Occopus must be enabled,  and the interface must be set to Occopus as in the example below.</p> <pre><code>YOUR-VIRTUAL-MACHINE:\ntype: tosca.nodes.MiCADO.CloudSigma.Compute\nproperties:\nnum_cpus: ADD_NUM_CPUS_FREQ (e.g. 4096)\nmem_size: ADD_MEM_SIZE (e.g. 4294967296)\nvnc_password: ADD_YOUR_PW (e.g. secret)\nlibdrive_id: ADD_YOUR_ID_HERE (eg. 87ce928e-e0bc-4cab-9502-514e523783e3)\npublic_key_id: ADD_YOUR_ID_HERE (e.g. d7c0f1ee-40df-4029-8d95-ec35b34dae1e)\nnics:\n- firewall_policy: ADD_YOUR_FIREWALL_POLICY_ID_HERE (e.g. fd97e326-83c8-44d8-90f7-0a19110f3c9d)\nip_v4_conf:\nconf: dhcp\n\ninterfaces:\nOccopus:\ncreate:\ninputs:\nendpoint: ADD_YOUR_ENDPOINT (e.g for cloudsigma https://zrh.cloudsigma.com/api/2.0 )\n</code></pre> <p>Under the properties section of a CloudSigma virtual machine definition these inputs are available.:</p> <ul> <li>num_cpus is the speed of CPU (e.g.\u00a04096) in terms of MHz of your VM   to be instantiated. The CPU frequency required to be between 250 and 100000</li> <li>mem_size is the amount of RAM (e.g.\u00a04294967296) in terms of bytes to be   allocated for your VM. The memory required to be between 268435456 and   137438953472</li> <li>vnc_password set the password for your VNC session (e.g.\u00a0secret).</li> <li>libdrive_id is the image id (e.g.\u00a087ce928e-e0bc-4cab-9502-514e523783e3)   on your CloudSigma cloud. Select an image containing a base os installation   with cloud-init support!</li> <li>public_key_id specifies the keypairs   (e.g.\u00a0d7c0f1ee-40df-4029-8d95-ec35b34dae1e) to be assigned to your VM.</li> <li>nics[.firewall_policy &amp;&amp; .ip_v4_conf.conf]  specifies network policies   (you can define multiple security groups in the form of a list for your VM).</li> </ul>"},{"location":"adt/vms/cloudsigma/#interfaces","title":"Interfaces","text":"<p>Under the interfaces section of a CloudSigma virtual machine definition, remember to pass the endpoint of the CloudSigma region you wish to provision in.</p>"},{"location":"adt/vms/ec2/","title":"AWS EC2","text":""},{"location":"adt/vms/ec2/#overview","title":"Overview","text":"<p>To instantiate MiCADO workers on a cloud through EC2 interface, please use the template below. MiCADO requires region_name, image_id and instance_type to instantiate a VM through EC2.</p> <p>Terraform supports provisioning on AWS EC2, and Occopus supports both AWS EC2 and OpenNebula EC2. To use Terraform, enable it and adjust the interfaces section accordingly.</p> <pre><code>YOUR-VIRTUAL-MACHINE:\ntype: tosca.nodes.MiCADO.EC2.Compute\nproperties:\nregion_name: ADD_YOUR_REGION_NAME_HERE (e.g. eu-west-1)\nimage_id: ADD_YOUR_ID_HERE (e.g. ami-12345678)\ninstance_type: ADD_YOUR_INSTANCE_TYPE_HERE (e.g. t1.small)\n\ninterfaces:\nOccopus:\ncreate:\ninputs:\nendpoint: ADD_YOUR_ENDPOINT (e.g https://ec2.eu-west-1.amazonaws.com)\n</code></pre>"},{"location":"adt/vms/ec2/#properties","title":"Properties","text":"<p>Under the properties section of an EC2 virtual machine definition these inputs are available.:</p> <ul> <li>region_name is the region name within an EC2 cloud (e.g.\u00a0eu-west-1).</li> <li>image_id is the image id (e.g.\u00a0ami-12345678) on your EC2 cloud. Select an   image containing a base os installation with cloud-init support!</li> <li>instance_type is the instance type (e.g.\u00a0t1.small) of your VM to be   instantiated.</li> <li>key_name optionally specifies the keypair (e.g.\u00a0my_ssh_keypair) to be   deployed on your VM.</li> <li>security_group_ids optionally specify security settings (you can define   multiple security groups or just one, but this property must be formatted as   a list, e.g. [sg-93d46bf7]) of your VM.</li> <li>subnet_id optionally specifies subnet identifier (e.g.\u00a0subnet-644e1e13)   to be attached to the VM.</li> </ul>"},{"location":"adt/vms/ec2/#interfaces","title":"Interfaces","text":"<p>Under the interfaces section of an EC2 virtual machine definition, the endpoint input is required by Occopus as seen in the example above.</p> <p>For Terraform the endpoint is discovered automatically based on region. To customise the endpoint pass the endpoint input in interfaces.</p> <pre><code>...\ninterfaces:\nTerraform:\ncreate:\ninputs:\nendpoint: ADD_YOUR_ENDPOINT (e.g https://my-custom-endpoint/api)\n</code></pre>"},{"location":"adt/vms/google/","title":"Google Compute Engine","text":""},{"location":"adt/vms/google/#overview","title":"Overview","text":"<p>To instantiate MiCADO workers on a cloud through Google interface, please use the template below. Currently, only Terraform has support for Google Cloud, so Terraform must be enabled, and the interface must be set to Terraform as in the example below.</p> <pre><code>YOUR-VIRTUAL-MACHINE:\ntype: tosca.nodes.MiCADO.GCE.Compute\nproperties:\nregion: ADD_YOUR_ID_HERE (e.g. us-west1)\nzone: ADD_YOUR_ID_HERE (e.g. us-west1-a)\nproject: ADD_YOUR_ID_HERE (e.g. PGCE)\nmachine_type: ADD_YOUR_ID_HERE (e.g. n1-standard-2)\nimage: ADD_YOUR_ID_HERE (e.g.  ubuntu-os-cloud/ubuntu-1804-lts)\nnetwork: ADD_YOUR_ID_HERE (e.g. default)\nssh-keys: ADD_YOUR_ID_HERE (e.g. ssh-rsa AAAB3N...)\n\ninterfaces:\nTerraform:\ncreate:\n</code></pre>"},{"location":"adt/vms/google/#properties","title":"Properties","text":"<p>Under the properties section of a GCE virtual machine definition these inputs are available.:</p> <ul> <li>project is the project to manage the resources in.</li> <li>image specifies the image from which to initialize the VM disk.</li> <li>region is the region that the resources should be created in.</li> <li>machine_type specifies the type of machine to create.</li> <li>zone is the zone that the machine should be created in.</li> <li>network is the network to attach to the instance.</li> <li>ssh-keys sets the public SSH key to be associated with the instance.</li> </ul>"},{"location":"adt/vms/google/#interfaces","title":"Interfaces","text":"<p>Under the interfaces section of a GCE virtual machine definition no specific inputs are required, but Terraform: create: should be present</p>"},{"location":"adt/vms/google/#authentication","title":"Authentication","text":"<p>Authentication in GCE is done using a service account key file in JSON format. You can manage the key files using the Cloud Console. The steps to retrieve the key file is as follows :</p> <ul> <li>Open the IAM &amp; Admin page in the Cloud Console.</li> <li>Click Select a project, choose a project, and click Open.</li> <li>In the left nav, click Service accounts.</li> <li>Find the row of the service account that you want to create a key for.     In that row, click the More button, and then click Create key.</li> <li>Select a Key type and click Create.</li> </ul>"},{"location":"adt/vms/openstack/","title":"OpenStack (Nova)","text":""},{"location":"adt/vms/openstack/#overview","title":"Overview","text":"<p>To instantiate MiCADO workers on a cloud through Nova interface, please use the template below. MiCADO requires image_id, flavor_name, project_id and network_id to instantiate a VM through Nova.</p> <p>Both Occopus and Terraform support Nova provisioning. To use Terraform, enable it and adjust the interfaces section accordingly.</p> <pre><code>YOUR-VIRTUAL-MACHINE:\ntype: tosca.nodes.MiCADO.Nova.Compute\nproperties:\nimage_id: ADD_YOUR_ID_HERE (e.g. d4f4e496-031a-4f49-b034-f8dafe28e01c)\nflavor_name: ADD_YOUR_ID_HERE (e.g. 3)\nproject_id: ADD_YOUR_ID_HERE (e.g. a678d20e71cb4b9f812a31e5f3eb63b0)\nnetwork_id: ADD_YOUR_ID_HERE (e.g. 3fd4c62d-5fbe-4bd9-9a9f-c161dabeefde)\nkey_name: ADD_YOUR_KEY_HERE (e.g. keyname)\nsecurity_groups:\n- ADD_YOUR_ID_HERE (e.g. d509348f-21f1-4723-9475-0cf749e05c33)\n\ninterfaces:\nOccopus:\ncreate:\ninputs:\nendpoint: ADD_YOUR_ENDPOINT (e.g https://sztaki.cloud.mta.hu:5000/v3)\n</code></pre>"},{"location":"adt/vms/openstack/#properties","title":"Properties","text":"<p>Under the properties section of a Nova virtual machine definition these inputs are available.:</p> <ul> <li>project_id is the id of project you would like to use on your target   Nova cloud.</li> <li>image_id is the image id on your Nova cloud. Select an image containing   a base os installation with cloud-init support!</li> <li>flavor_name is the id of the desired flavor for the VM.</li> <li>tenant_name is the name of the Tenant or Project to login with.</li> <li>user_domain_name is the domain name where the user is located.</li> <li>availability_zone is the availability zone in which to create the VM.</li> <li>server_name optionally defines the hostname of VM (e.g.:\u201dhelloworld\u201d).</li> <li>key_name optionally sets the name of the keypair to be associated to the   instance. Keypair name must be defined on the target nova cloud before   launching the VM.</li> <li>security_groups optionally specify security settings (you can define   multiple security groups in the form of a list) for your VM.</li> <li>network_id is the id of the network you would like to use on your target   Nova cloud.</li> <li>floating_ip_pool (Terraform only) is a string specifying the pool of floating   IPs that this instance should be assigned a random available floating IP from. If   this property is not specified, the instance will not be assigned a floating IP.</li> <li>floating_ip (Terraform only) is a string specifying the specific floating IP   from the above specified pool that this instance should have assigned to it. This   property should not be used with instances that may scale out to more than one replica.</li> <li>config_drive (Terraform only) is a boolean to enable use of a configuration   drive for metadata storage.</li> </ul>"},{"location":"adt/vms/openstack/#interfaces","title":"Interfaces","text":"<p>Under the interfaces section of a Nova virtual machine definition, the endpoint input (v3 Identity service) is required as seen in the example above.</p> <p>For Terraform the endpoint should also be passed as endpoint  in inputs. Depending on the configuration of the OpenStack cluster, it may be necessary to provide network_name in addition to the ID.</p> <pre><code>...\ninterfaces:\nTerraform:\ncreate:\ninputs:\nendpoint: ADD_YOUR_ENDPOINT (e.g https://sztaki.cloud.mta.hu:5000/v3)\nnetwork_name: ADD_YOUR_NETWORK_NAME (e.g mynet-default)\n</code></pre>"},{"location":"adt/vms/openstack/#authentication","title":"Authentication","text":"<p>Authentication in OpenStack is supported by MiCADO in three ways, by specifying the appropriate fields during cloud credential configuration.</p> <ul> <li> <p>The default method is authenticating with the same credentials   used to access the OpenStack WebUI by providing   the username and password fields during   configuration.</p> </li> <li> <p>Another option is with   Application Credentials   For this method, provide application_credential_id and   applicaiton_credential_secret when configuring.   If these fields are filled, username and password will be   ignored.</p> </li> <li> <p>A third option is with OpenID Connect   for which the URL of the OpenID provider (identity_provider) and   a valid access_token are required. When providing a literal access   token is not practical (for example   with short-lived access tokens), MiCADO supports automatically   refreshing access tokens   First, complete the <code>openid</code> section under <code>pre-authentication</code> with a   url, client_id, client_secret and valid refresh_token.   Then, for the value of access_token use the following value: <code>*OPENID</code>.</p> </li> </ul>"},{"location":"adt/vms/oracle/","title":"Oracle Cloud Infrastructure","text":""},{"location":"adt/vms/oracle/#overview","title":"Overview","text":"<p>To instantiate MiCADO workers on a cloud through Oracle interface, please use the template below. Currently, only Terraform has support for Oracle, so Terraform must be enabled, and the interface must be set to Terraform as in the example below.</p> <p>Note that OCI's Ubuntu VM images feature a number of strict <code>iptables</code> rules, which will restrict normal communnication between worker nodes and the MiCADO Control Plane. To resolve this issue, it is important to include the VM contextualisation commands that can be seen in the example below.</p> <pre><code>YOUR-VIRTUAL-MACHINE:\ntype: tosca.nodes.MiCADO.OCI.Compute\nproperties:\nregion: &lt;REGION_NAME&gt; (e.g. uk-london-1)\navailability_domain: &lt;AVAILABILITY_DOMAIN&gt; (e.g. lVvK:UK-LONDON-1-AD-1)\ncompartment_id: &lt;COMPARTMENT_OCID&gt; (e.g ocid1.tenancy.oc1..aaa)\nshape: &lt;VM_TYPE_NAME&gt; (e.g. VM.Standard.E2.1)\nsource_id: &lt;VM_IMAGE_OCID&gt; (e.g ocid1.image.oc1.uk-london-1.aaa)\nsubnet_id: &lt;SUBNET_OCID&gt; (e.g ocid1.subnet.oc1.uk-london-1.aaa)\nnetwork_security_group: &lt;NETWORK_SECURITY_GROUP_OCID&gt; (e.g ocid1.networksecuritygroup.oc1.uk-london-1.aaa)\nssh_keys: ADD_YOUR_ID_HERE (e.g. ssh-rsa AAAB3N...)\ncontext:\ninsert: true\ncloud_config: |\nruncmd:\n- iptables -D INPUT -j REJECT --reject-with icmp-host-prohibited\n- iptables -D FORWARD -j REJECT --reject-with icmp-host-prohibited\n\ninterfaces:\nTerraform:\ncreate:\n</code></pre>"},{"location":"adt/vms/oracle/#properties","title":"Properties","text":"<p>Under the properties section of a OCI virtual machine definition these inputs are available.:</p> <ul> <li>availability_domain is the availability domain of the instance.</li> <li>source_id specifies the OCID of an image from which to initialize the   VM disk.</li> <li>region is the region that the resources should be created in.</li> <li>shape specifies the type of machine to create.</li> <li>compartment_id is the OCID of the compartment.</li> <li>subnet_id is the OCID of the subnet to create the VNIC in.</li> <li>network_security_group specifies the OCID of the network security   settings for the VM.</li> <li>ssh_keys sets the public SSH key to be associated with the instance.</li> </ul>"},{"location":"adt/vms/oracle/#interfaces","title":"Interfaces","text":"<p>Under the interfaces section of an OCI virtual machine definition no specific inputs are required, but Terraform: create: should be present.</p>"},{"location":"adt/vms/oracle/#authentication","title":"Authentication","text":"<p>Authentication in OCI is supported by MiCADO in two ways:</p> <ul> <li> <p>The first is by setting up an Instance Principal   based authentication on the MiCADO Control Plane by creating suitable Dynamic Group and Policies   associated with it.</p> </li> <li> <p>The other option is by enabling an   API Key   based authentication on the MiCADO Control Plane and providing the required   fields during cloud credential configuration.</p> </li> </ul>"},{"location":"changelog/","title":"What's New, What's Coming","text":""},{"location":"changelog/#v012x","title":"v0.12.x","text":""},{"location":"changelog/#major-enhancements","title":"Major Enhancements","text":""},{"location":"changelog/#k3s-under-the-hood","title":"K3s Under-the-Hood","text":"<p>This release will introduce K3s as the underlying Kubernetes distribution for MiCADO.</p>"},{"location":"changelog/#v011x","title":"v0.11.x","text":""},{"location":"changelog/#major-enhancements_1","title":"Major Enhancements","text":""},{"location":"changelog/#ansible-runner-support","title":"Ansible-Runner support","text":"<p>This release introduces support for deployment via the Ansible Runner Python package, which provides a Python interface to Ansible Playbooks. While not a major user-facing change, this enhancement greatly improves the stability of <code>micado-client</code> and paves the way for user installations with a command-line interface.</p>"},{"location":"changelog/#ubuntu-2204-support","title":"Ubuntu 22.04 Support","text":"<p>Ubuntu 22.04 LTS is now the recommended OS for MiCADO!</p>"},{"location":"changelog/#fixes","title":"Fixes","text":""},{"location":"changelog/#iptables-service-issues","title":"IPtables service issues","text":"<p>A rare race condition would see iptables services fail to start and roll-out new iptables rules. This has been fixed.</p>"},{"location":"changelog/#known-issues-deprecations","title":"Known Issues &amp; Deprecations","text":""},{"location":"changelog/#kubeapi-issues","title":"KubeAPI Issues","text":"<p>There are infrequent reports of the Kubernetes API failing to properly start during deployment with kubeadm, resulting in a failed deployment. Re-running the Playbook fixes the issue, but we are looking at a proper fix for programmatic installations.</p>"},{"location":"changelog/#v0100","title":"v0.10.0","text":""},{"location":"changelog/#major-enhancements_2","title":"Major Enhancements","text":"<p>This is the public release of the now merged MiCADO-Edge. See those changes detailed below. There are only a few additional notes for this release.</p>"},{"location":"changelog/#fixes_1","title":"Fixes","text":""},{"location":"changelog/#openstack-ssh-keypair-name-collision","title":"OpenStack SSH Keypair name collision","text":"<p>If multiple applications were being launched by different users on the same OpenStack cluster, there was a possibility that different SSH Keypair names generated by MiCADO would have the same name. This has now been corrected.</p>"},{"location":"changelog/#known-issues-deprecations_1","title":"Known Issues &amp; Deprecations","text":""},{"location":"changelog/#ubuntu-1604","title":"Ubuntu 16.04","text":"<p>Support for Ubuntu 16.04 LTS has been deprecated. We are currently supporting the long-term support releases of Ubuntu: 18.04 and 20.04</p>"},{"location":"changelog/#micado-edge-internal","title":"MiCADO-Edge (Internal)","text":""},{"location":"changelog/#major-enhancements_3","title":"Major Enhancements","text":""},{"location":"changelog/#support-for-edge-nodes","title":"Support for Edge Nodes","text":"<p>MiCADO now includes support for adding edge nodes to the cluster via KubeEdge. Both x86 and ARM architectures are supported and current Debian-based (including Raspbian) distributions of Linux have been tested. Connecting Edges is done via the Ansible Playbook used to deploy MiCADO. Once connected, an ADT can reference these edges to ensure application components are deployed appropriately.</p>"},{"location":"changelog/#more-authentication-types-supported-in-openstack","title":"More Authentication Types Supported in OpenStack","text":"<p>MiCADO now supports OpenID Connect authentication in OpenStack with Keystone's <code>v3.OidcAccessToken</code>. MiCADO can generate the access token when required, given a refresh token and client authentication details. These credentials should be entered in the normal way, during credential configuration</p> <p>The existing <code>v3.Password</code> authentication has been extended with the option of specifying the User Domain - where necessary.</p>"},{"location":"changelog/#support-for-specifying-floating-ips-in-openstack","title":"Support for Specifying Floating IPs in OpenStack","text":"<p>Users can now specify the desired floating IP for their instance when authoring an ADT. The floating IP must exist in the specified IP pool, and must not be already allocated.</p>"},{"location":"changelog/#v091","title":"v0.9.1","text":""},{"location":"changelog/#major-enhancements_4","title":"Major Enhancements","text":""},{"location":"changelog/#support-for-oracle-cloud-infrastructure","title":"Support for Oracle Cloud Infrastructure","text":"<p>MiCADO now includes support for Oracle Cloud Infrastructure!</p> <p>Provisioning virtual machines in OCI is currently supported by Terraform in MiCADO. Details on preparing your ADT for use with Oracle can be found in the relevant section of the documentation.</p>"},{"location":"changelog/#submitter-api-v20","title":"Submitter API v2.0","text":"<p>The MiCADO Submitter RESTful API has been updated to v2.0, to facilitate better intergration with other tools and platforms. The functionality of the previous API version has been preserved and limited backwards compatibility with v1.0 of the API is still maintained in this version.</p> <p>Expect v1.0 of the API to be deprecated in a future version.</p>"},{"location":"changelog/#fixes_2","title":"Fixes","text":""},{"location":"changelog/#kubernetes-secret-distribution-via-adt-policies","title":"Kubernetes Secret Distribution via ADT Policies","text":"<p>A bug which prevented MiCADO from distributing application secrets defined in policies within the ADT has been resolved. See the relevant section of the documentation for details on how to define and assign secrets inside an ADT.</p>"},{"location":"changelog/#known-issues-deprecations_2","title":"Known Issues &amp; Deprecations","text":""},{"location":"changelog/#ubuntu-1604_1","title":"Ubuntu 16.04","text":"<p>This version of MiCADO adds support for the latest Ubuntu 20.04 LTS. Going forward, MiCADO will deprecate support for Ubuntu 16.04 LTS and focus on supporting the current long-term support releases of Ubuntu: 18.04 and 20.04</p>"},{"location":"changelog/#v090","title":"v0.9.0","text":""},{"location":"changelog/#major-enhancements_5","title":"Major Enhancements","text":""},{"location":"changelog/#terraform-for-cloud-orchestration","title":"Terraform for Cloud Orchestration","text":"<p>Support for Terraform has been added to MiCADO! The TerraformAdaptor currently supports the following cloud resources:</p> <ul> <li>OpenStack Nova Compute</li> <li>Amazon EC2 Compute</li> <li>Microsoft Azure Compute</li> <li>Google Compute Engine</li> </ul> <p>To use Terraform with MiCADO it must be enabled during deployment of the MiCADO Control Plane, and an appropriate ADT should be used.</p>"},{"location":"changelog/#improved-credential-file-handling","title":"Improved Credential File Handling","text":"<p>Cloud credentials are now stored in Kubernetes Secrets on the MiCADO Control Plane. Additionally, credentials on an already deployed MiCADO can now be updated or modified using Ansible.</p>"},{"location":"changelog/#improved-node-contextualisation","title":"Improved Node Contextualisation","text":"<p>It is now possible to insert contextualisation configurations earlier in the default cloud-init #cloud-config for worker nodes. This extends the existing append functionality to support configuration tasks which should precede the initialisation of the worker node (joining the Kubernetes cluster, bringing up the IPSec tunnel, etc...)</p>"},{"location":"changelog/#fixes_3","title":"Fixes","text":""},{"location":"changelog/#zorp-ingress","title":"Zorp Ingress","text":"<p>The Zorp Ingress Controllers in v0.8.0 were incorrectly being deployed alongside every application, even if the policy did not call for it. This has now been resolved.</p> <p>Additionally, these workers were requesting a large amount of CPU and Memory, which could limit scheduling on the node. Those requests have been lowered to more reasonable values.</p>"},{"location":"changelog/#different-versioned-workers","title":"Different Versioned Workers","text":"<p>In previous versions of MiCADO, deployed worker nodes which did not match the Ubuntu version of the MiCADO Control Plane would be unable to join the MiCADO cluster. This has now been resolved.</p>"},{"location":"changelog/#known-issues-deprecations_3","title":"Known Issues &amp; Deprecations","text":""},{"location":"changelog/#ipsec-and-dropped-network-packets","title":"IPSec and Dropped Network Packets","text":"<p>On some network configurations, for example where IPSec protocols ESP (50) and AH (51) are blocked, important network packets can get dropped in Master-Worker communications. This might be seen as Prometheus scrapes failing with the error context deadline exceeded, or Workers failing to join the Kubernetes cluster. To disable the IPSec tunnel securing Master-Worker communications, it can be stopped by appending ipsec stop to runcmd in the default worker node cloud-init #cloud-config.</p>"},{"location":"changelog/#compute-node-inputs-in-adts","title":"Compute Node Inputs in ADTs","text":"<p>The Occopus input interface_cloud has been deprecated and removed, as cloud discovery is now based on TOSCA type. It will continue to be supported (ignored) in this version of MiCADO but may raise warnings or errors in future versions.</p> <p>The input endpoint_cloud has been deprecated in favour of endpoint. Both Terraform and Occopus will support endpoint_cloud in this version of MiCADO but a future version will drop support.</p> <p>With the above changes in mind, Terraform will support v0.8.0 ADTs which only include EC2 or Nova Compute nodes. This can be acheieved simply by changing interfaces from Occopus to Terraform, though it should be noted:</p> <ul> <li>Terraform will auto-discover the EC2 endpoint based on the region_name   property, making the endpoint input no longer required. The endpoint   input can still be passed in to provide a custom endpoint.</li> <li>For some OpenStack configurations, Terraform requires a network_name   as well as network_id to correctly identify networks. The network_name   property can be passed in as properties or inputs</li> </ul>"},{"location":"changelog/changelog/","title":"Release Notes","text":""},{"location":"changelog/changelog/#v0120-rc1-6-march-2023","title":"v0.12.0-rc1 (6 March 2023)","text":"<ul> <li>Change the underlying Kubernetes distro to K3s</li> </ul>"},{"location":"changelog/changelog/#v0114-21-january-2023","title":"v0.11.4 (21 January 2023)","text":"<ul> <li>HOTFIX for flannel issues on Ubuntu versions lower than 22.04</li> </ul>"},{"location":"changelog/changelog/#v0113-4-october-2022","title":"v0.11.3 (4 October 2022)","text":"<ul> <li>support multiple docker logins by @jaydesl</li> </ul>"},{"location":"changelog/changelog/#v01112-27-september-2022","title":"v0.11.1/2 (27 September 2022)","text":"<ul> <li>Fix a rare race condition in rebooting the iptables service</li> <li>Add full support for Ubuntu 22.04</li> <li>Extend timeout for ansible-runner installations</li> </ul>"},{"location":"changelog/changelog/#v0110-26-september-2022","title":"v0.11.0 (26 September 2022)","text":"<ul> <li>Organise for a cleaner repository structure by @jaydesl</li> <li>Bump Python packages and MiCADO components by @hmdcpc2021</li> <li>Fix a permission issue, where users needed a password for sudo by @maystery</li> <li>Re-structure directory structure for ansiblerunner by @jaydesl</li> <li>Add Occopus worker node IP info to dashboard by @jaydesl</li> <li>Add support for new EGI auth mechanism by @ResmiArjun</li> <li>Support installation with ansible-runner</li> </ul>"},{"location":"changelog/changelog/#v0100-23-february-2022","title":"v0.10.0 (23 February 2022)","text":"<ul> <li>Make MiCADO-Edge release public</li> <li>Support TOSCA v1.3 instance_count and occurrences for node replicas</li> <li>Support the Kubernetes subpath feature for multi volume mounting</li> <li>Update Docker, containerd, Kubernetes and KubeEdge versions</li> <li>Fix an issue with SSH keypair name collisions in OpenStack</li> </ul>"},{"location":"changelog/changelog/#micado-edge-4-may-2021","title":"MiCADO-Edge (4 May 2021)","text":"<ul> <li>This \"release\" quietly merged the separate MiCADO-Edge feature branch</li> <li>Add support for Edge nodes with <code>KubeEdge &lt;https://kubeedge.io&gt;</code>__</li> <li>Add support for OpenID Connect Authentication in OpenStack</li> <li>Add support for specifying the floating IP of an instance</li> <li>Fix an issue where Playbooks would fail on first <code>apt-get</code></li> <li>Add support for drop-in Ansible plays</li> <li>Organise Playbook as per Ansible best practices</li> <li>Set best practice configuration with ansible.cfg</li> <li>Support domain_name property for v3 Password Authentication</li> </ul>"},{"location":"changelog/changelog/#v091-rev1-28-october-2020","title":"v0.9.1-rev1 (28 October 2020)","text":"<ul> <li>Fix an issue where Kubernetes playbook tasks would fail (<code>ansible-collections/community.kubernetes/issues/273 &lt;https://github.com/ansible-collections/community.kubernetes/issues/273&gt;</code>__)</li> </ul>"},{"location":"changelog/changelog/#v091-1-october-2020","title":"v0.9.1 (1 October 2020)","text":"<ul> <li>Add support for Oracle Cloud Infrastructure</li> <li>Add support for Ubuntu 20.04 LTS</li> <li>Improve RESTful nature of Submitter with v2.0 API</li> <li>Base component images on <code>alpine</code> for a smaller footprint</li> <li>Bump Kubernetes to v1.19</li> <li>Support TOSCA v1.2 template files</li> <li>Refactor custom TOSCA type definitions</li> <li>Refactor Submitter parsing modules to improve parsing times</li> <li>Refactor KubernetesAdaptor for more customisable resources</li> <li>Improve validation of translated Kubernetes manifests</li> <li>Support config_drive flag in OpenStack (Terraform only)</li> <li>Port PolicyKeeper to Python3</li> <li>Increase timeout for MiCADO component deployment (for slower machines)</li> <li>Increase timeout for inactive worker node removal (for poor networks)</li> <li>Reduce Prometheus default scrape interval (for custom exporters)</li> <li>Add <code>mode</code> to Ansible tasks for CVE-2020-1736</li> <li>Include hostname and IP as SANs in self-signed certs</li> <li>Fix: enable secret distribution via ADT policy</li> </ul>"},{"location":"changelog/changelog/#v090-9-april-2020","title":"v0.9.0 (9 April 2020)","text":"<ul> <li>Refactor playbook tasks to be more component-specific</li> <li>Add playbook tasks for configuring and installing Terraform</li> <li>Use the Ansible k8s module for managing Kubernetes resources</li> <li>Optimise cloud-init scripts by reducing apt-get update</li> <li>Fix Master-Worker Ubuntu mismatch bug</li> <li>Handle undefined credential file path</li> <li>Store credential data in Kubernetes Secrets</li> <li>Support updates of credentials on a deployed MiCADO Control Plane</li> <li>Add demo ADTs for Azure &amp; GCE</li> <li>Update QuickStart docs in README</li> <li>Bump Grafana to v6.6.2</li> <li>Bump Prometheus to v2.16.0</li> <li>Bump Kubernetes-Dashboard to v2.0.0 (rc7)</li> <li>Hide Kubernetes Secrets on Kubernetes-Dashboard</li> <li>Refactor PK main loop to support multiple cloud orchestrators</li> <li>Add Terraform handler to PK for scaling (up/down and dropping specific nodes)</li> <li>Switch to the pykube package in PK instead of kubernetes</li> <li>Add the TerraformAdaptor to the TOSCASubmitter</li> <li>Bump TOSCASubmitter package versions</li> <li>Discover cloud from TOSCA ADT type and deprecate interface_cloud</li> <li>Rename ADT compute property endpoint_cloud to endpoint</li> <li>Support insert in ADT to modify cloud-init cloud-config</li> <li>Support authentication with OpenStack application credential</li> <li>Pass orchestrator info to PK during PKAdaptor translation</li> <li>Lower reserved CPU and Memory for Zorp Ingress on workers</li> <li>Only deploy Zorp Ingress to workers with matching ADT policy</li> <li>Bump Kubernetes to v1.18</li> <li>Bump Flannel to v0.12</li> <li>Bump containerd.io to v.1.2.13</li> <li>Bump Occopus to v1.7 (rc6)</li> <li>Bump cAdvisor to v0.34.0</li> <li>Bump AlertManager to v0.20.0</li> </ul>"},{"location":"changelog/changelog/#v080-30-september-2019","title":"v0.8.0 (30 September 2019)","text":"<ul> <li>simplify ADTs by introducing pre-defined TOSCA node types</li> <li>add support for Kubernetes ConfigMaps, Namespaces and multi-container Pods</li> <li>metric collection (disabled by default) is now enabled with \"monitoring\" policy</li> <li>upgrade all components (Docker, Kubernetes, Grafana, Prometheus, etc...)</li> <li>introduce new Optimizer supported scaling</li> <li>add MiCADO version on dashboard and Grafana</li> <li>introduce log rotate for Docker and components</li> <li>introduce node downscale mechanism with node selection</li> <li>redirect stdout of scaling_rule usercode to different log file</li> <li>add support of keystone V3 for OpenStack in Occopus</li> <li>improve cloud API handling in Occopus</li> <li>make the master node web authentication timeout configurable</li> <li>make master-worker node VPN connection more restrictive</li> <li>implement ADT-based application secret distribution</li> <li>push cloud secrets to Credential Store at deploy time</li> <li>implement Security Policy Manager adaptor in the TOSCA Submitter</li> <li>add support for configuring application-level firewalling rules for the application through the ADT (FWaaS)</li> <li>generate node certificate with the right common name for the master node</li> <li>make the micadoctl command line utility to work after the transition to Kubernetes pods</li> <li>fix keypair distribution to worker nodes</li> <li>update TOSCA template for Kubernetes application-level secret distribution</li> <li>refactor Kubernetes translation</li> <li>fix Policy Keeper Kubernetes node maintenance</li> <li>propagate Kubelet configuration to woker nodes</li> <li>support system cGroup driver by Docker &amp; Kubernetes</li> <li>fix Kubernetes node objects to be deleted on \"undeploy\"</li> <li>fix Occopus create &amp; import actions to correctly raise exceptions</li> <li>fix Occopus updates not to kill unrelated nodes</li> <li>support updates of an ADT with no Occopus nodes</li> <li>support updates of an ADT with no Kubernetes nodes</li> <li>add a timeout to Kubernetes undeploy</li> <li>simplify hosts.yml file</li> </ul>"},{"location":"changelog/changelog/#v073-14-jun-2019","title":"v0.7.3 (14 Jun 2019)","text":"<ul> <li>update MiCADO internal core services to run in Kubernetes pods</li> <li>remove Consul and replace it with Prometheus\u2019 Kubernetes Service Discovery</li> <li>update cAdvisor and NodeExporter to run as Kubernetes DaemonSets</li> <li>introduce the support for creating prepared image for the MiCADO Control Plane and the MiCADO worker</li> <li>introduce the support for deploying unique \u201csets\u201d of virtual machines scaling independently</li> <li>update Grafana to track the independently scaling VMs from the drop-down Node ID</li> <li>update scrape interval between Prometheus and cAdvisor to be less frequent</li> <li>fix the Occopus Adaptor to correctly raise exceptions for the submitter</li> <li>update Kubernetes Dashboard to improve RBAC permissions</li> <li>update the Flannel Overlay deployment</li> <li>update the Kubernetes eviction thresholds on the Master node to be lowered</li> <li>remove Docker-Compose from Master &amp; Workers</li> <li>fix dependencies and vulnerabilities</li> <li>add dry-run support for the Submitter upon launch of TOSCA ADT</li> <li>add new api call for the Submitter to validate TOSCA template</li> <li>improve Submitter logs</li> <li>improve Submitter responses to users</li> <li>improve handling of wrong template by Submitter</li> <li>add support for hv_relaxed and hv_tsc CloudSigma specific properties</li> <li>add support for tagging EC2 type resources</li> <li>add disk and free space checking to the deployment playbook</li> <li>update the Wordpress demo to demonstrate \u201cvirtual machine sets\u201d</li> <li>update the cQueue demo to demonstrate \u201cvirtual machine sets\u201d</li> <li>fix and improve the NGINX demo</li> </ul>"},{"location":"changelog/changelog/#v072-rev1-01-apr-2019","title":"v0.7.2-rev1 (01 Apr 2019)","text":"<ul> <li>fix dependency issue for Kubernetes 1.13.1 (<code>kubernetes/kubernetes#75683 &lt;https://github.com/kubernetes/kubernetes/issues/75683&gt;</code>__)</li> </ul>"},{"location":"changelog/changelog/#v072-25-feb-2019","title":"v0.7.2 (25 Feb 2019)","text":"<ul> <li>add checking for minimal memory on MiCADO Control Plane at deployment</li> <li>support private networks on cloudsigma</li> <li>support user-defined contextualisation</li> <li>support re-use across other container &amp; cloud orchestrators in ADT</li> <li>new TOSCA to Kubernetes Manifest Adaptor</li> <li>add support for creating DaemonSets, Jobs, StatefulSets (with limited functionality) and standalone Pods</li> <li>add support for creating PersistentVolumes &amp; PVClaims</li> <li>add support for specifying custom service details (NodePort, ClusterIP, etc.)</li> <li>minor improvements to Grafana dashboard</li> <li>support asynchronous calls through TOSCASubmitter API</li> <li>fix kubectl error on MiCADO Control Plane restart</li> <li>fix TOSCASubmitter rollback on errors</li> <li>fix TOSCASubmitter status &amp; output display</li> <li>add support for encrypting master-worker communication</li> <li>automatically provision and revoke security credentials for worker nodes</li> <li>update default MTU to 1400 to ensure compatibility with OpenStack and AWS</li> <li>add Credential Store security enabler</li> <li>add Security Policy Manager security enabler</li> <li>add Image Integrity Verifier Security enabler</li> <li>add Crypto Engine security enabler</li> <li>add support for kubernetes secrets</li> <li>reimplement Credential Manager using the flask-users library</li> </ul>"},{"location":"changelog/changelog/#v071-10-jan-2019","title":"v0.7.1 (10 Jan 2019)","text":"<ul> <li>Fix: Add SKIP back to Dashboard (defaults changed in v1.13.1)</li> <li>Fix: URL not found for Kubernetes manifest files</li> <li>Fix: Make sure worker node sets hostname correctly</li> <li>Fix: Don't update Kubernetes if template not changed</li> <li>Fix: Make playbook more idempotent</li> <li>Add Support for outputs via TOSCA ADT</li> <li>Add Kubernetes service discovery support to Prometheus</li> <li>Add new demo: nginx (HTTP request scaling)</li> </ul>"},{"location":"changelog/changelog/#v070-12-dec-2018","title":"v0.7.0 (12 Dec 2018)","text":"<ul> <li>Introduce Kubernetes as the primary container orchestration engine</li> <li>Replace the swarm-visualiser with the Kubernetes Dashboard</li> </ul>"},{"location":"changelog/changelog/#older-micado-versions","title":"Older MiCADO Versions","text":"<p>v0.6.1 (15 Oct 2018)</p> <ul> <li>enable VM-only deployments</li> <li>add support for special characters in SSL credentials</li> <li>fix missing vm instance number reset at undeployment</li> <li>add option to disable auto-updates on worker nodes</li> <li>modify default launch-order of TOSCA adaptors</li> <li>add cloud-specific TOSCA templates and improve helper scripts for stressng</li> <li>flatten CPU scaling policies</li> <li>improve virtual machine build time</li> <li>fix Zorp starting dependency</li> <li>fix Docker login timing issue</li> <li>remove unnecessary port from docker compose file</li> <li>enable Prometheus DB export</li> </ul> <p>v0.6.0 (10 Sept 2018)</p> <ul> <li>introduce documentation repository and host its content at http://micado-scale.readthedocs.io</li> <li>improve MiCADO Control Plane containers restart policy</li> <li>fix MTU issue in relation to Docker</li> <li>fix Occopus restart issue</li> <li>fix health-checking for Cloudbroker-AWS platform</li> <li>update host naming convention for worker and master nodes</li> <li>make wait-update task idempotent in ansible playbook</li> <li>fix issue with worker node deployment in EC2 clouds</li> <li>fix issue with user-defined Docker networks in OpenStack clouds</li> <li>make Submitter response message structure uniform</li> <li>add 'nodes' and 'services' query methods to REST API</li> <li>improve 'stressng' and 'cqueue' test helper scripts</li> <li>add more compose properties to custom TOSCA definition</li> <li>fix floating ip issues in the Dashboard component</li> <li>add new links to Dashboard to reflect the changes introduced by reverse proxying</li> <li>fix Dashboard to generate links based on the contents of the Host header to find the frontend URL automatically</li> <li>make consul security encryption based on generated random key instead of static key</li> <li>add reverse proxy, TLS encryption and application-level firewalling capabilities to the web interfaces exposed by the MiCADO Control Plane node</li> <li>add packet filtering for closing down non-public ports</li> <li>add systemd unit for MiCADO services</li> <li>update the ansible playbook to use the built-in service module for installing and handling MiCADO services</li> <li>update the documentation to reflect the changes after the introduction of reverse proxying</li> <li>add support for form-based authentication of exposed web services</li> <li>add COLA-themed login page</li> <li>add the Credential Manager component to store and handle web service users and passwords securely</li> <li>add support for provisioning a user to the Credential Manager via Ansible</li> <li>add support for user and admin roles in the Credential Manager</li> <li>add support for authorization of the web services based on user role</li> <li>add documentation about the Ansible Vault mechanism to protect sensitive deployment details</li> <li>add support for HTTP basic authentication for APIs</li> <li>add support for making the web interface's listening port configurable</li> <li>update the documentation of API calls in terms of authentication, encryption and reverse proxying</li> <li>add micadoctl tool for user and service management</li> <li>add HTTP method filter to firewall in order to control requests directed to containers</li> <li>add support for IPv6 exposure of services</li> <li>add IPv6 packet filtering</li> </ul> <p>v0.5.0 (12 July 2018)</p> <ul> <li>introduce supporting TOSCA</li> <li>introduce supporting user-defined scaling policy</li> <li>dashboard added with Docker Visualizer, Grafana, Prometheus</li> <li>deployment with Ansible playbook</li> <li>support private docker registry</li> <li>improve persistence of MiCADO Control Plane services</li> </ul>"},{"location":"demos/","title":"Demos","text":"<p>There are a number of demo applications that come bundled with MiCADO. After deploying MICADO you are welcome to try out the demos to get a feel for what MiCADO can do.</p> <p>Each demo works the same way - you must edit the ADT and provide cloud configuration details to describe an instance that will host the demo in the cloud. </p> <p>For the NGINX demo on EC2, you can open the ADT like so:</p> <pre><code>micado demo nginx ec2\n</code></pre> <p>To then run the demo, use the MiCADO CLI:</p> <pre><code>micado start nginx_ec2.yaml\n</code></pre> <p>Explore the Dashboard, and once you are done, shut down the demo with the MiCADO CLI:</p> <pre><code>micado stop\n</code></pre>"},{"location":"demos/stressng/","title":"stressng","text":"<p>This application deploys a single service -  <code>stressng</code>1 -  which exercises a constant CPU load on the system.</p> <p>A scaling policy is defined for this application that scales up/down both nodes and the stressng service based on CPU consumption. Helper scripts have been added to the directory to ease application handling.</p> <ol> <li> <p>In the container lorel/docker-stress-ng.\u00a0\u21a9</p> </li> </ol>"},{"location":"install/","title":"Installation","text":"<p>To deploy a MiCADO cluster, you need a virtual machine to be set up as the MiCADO control plane. We deploy a standard Kubernetes distribution (K3s) and a number of MiCADO-specific compoenents.</p> <p>Preparation of the control plane is facilitated by Ansible. If you are familiar with Ansible Playbooks, download the latest release and configure the relevant files before deployment.</p> <p>For those new to Ansible, we provide a small command-line tool to ease deployment of the MiCADO control plane.</p>"},{"location":"install/ansible-install/","title":"Install with Ansible","text":"<p>If you are familiar with Ansible and Ansible Playbooks, you are welcome to configure and run the Playbooks without the CLI. The steps below provide a rough outline to the required configurations.</p> <p>Warning</p> <p>This page is currently under development, and sections may be outdated or incorrect.</p>"},{"location":"install/ansible-install/#installation","title":"Installation","text":"<p>Perform the following steps either on your local machine or on MiCADO Control Plane depending on the installation method.</p>"},{"location":"install/ansible-install/#step-1-download-the-ansible-playbook","title":"Step 1: Download the ansible playbook","text":"<pre><code>curl --output ansible-micado.tar.gz -L https://github.com/micado-scale/ansible-micado/releases/download/v0.12.0/micado-v0.12.0.tar.gz\ntar -zxvf ansible-micado.tar.gz\n</code></pre>"},{"location":"install/ansible-install/#step-2-specify-cloud-credential-for-instantiating-micado-workers","title":"Step 2: Specify cloud credential for instantiating MiCADO workers.","text":"<p>MiCADO Control Plane will use the credentials against the cloud API to start/stop VM instances (MiCADO workers) to host the application and to realize scaling. Credentials here should belong to the same cloud as where MiCADO Control Plane is running. We recommend making a copy of our predefined template and edit it. MiCADO expects the credential in a file, called credentials-cloud-api.yml before deployment. Please, do not modify the structure of the template!</p> <pre><code>cd playbook/project/credentials\ncp sample-credentials-cloud-api.yml credentials-cloud-api.yml\nedit credentials-cloud-api.yml\n</code></pre> <p>Edit credentials-cloud-api.yml to add cloud credentials. You will find predefined sections in the template for each cloud interface type MiCADO supports. It is recommended to fill only the section belonging to your target cloud.</p> <p>NOTE If you are using Google Cloud, you must replace or fill the credentials-gce.json with your downloaded service account key file.</p> It is possible to modify cloud credentials after MiCADO has been deployed, see the section titled Update Cloud Credentials further down this page"},{"location":"install/ansible-install/#optional-added-security","title":"Optional: Added security","text":"<p>Credentials are stored in Kubernetes Secrets on the MiCADO Control Plane. If you wish to keep the credential data in an secure format on the Ansible Remote as well, you can use the Ansible Vault mechanism to to achieve this. Simply create the above file using Vault with the following command</p> <pre><code>ansible-vault create credentials-cloud-api.yml\n</code></pre> <p>This will launch the editor defined in the <code>$EDITOR</code> environment variable to make changes to the file. If you wish to make any changes to the previously encrypted file, you can use the command</p> <pre><code>ansible-vault edit credentials-cloud-api.yml\n</code></pre> <p>Be sure to see the note about deploying a playbook with vault encrypted files in Step 7.</p>"},{"location":"install/ansible-install/#step-3a-specify-security-settings-and-credentials-to-access-micado","title":"Step 3a: Specify security settings and credentials to access MiCADO","text":"<p>MiCADO Control Plane will use these security-related settings and credentials to authenticate its users for accessing the REST API and Dashboard.</p> <pre><code>cp sample-credentials-micado.yml credentials-micado.yml\nedit credentials-micado.yml\n</code></pre> Specify the provisioning method for the x509 keypair used for TLS encryption of the management interface in the <code>tls</code> subtree: <ul> <li> <p>The self-signed option generates a new keypair with the specified hostname as the subject / CN ('micado-master' by default, but configurable in micado-master.yml).</p> <p>Two Subject Alternative Name (SAN) entries are also added by the configuration file at <code>roles/micado_master/start/templates/zorp/san.cnf</code>:</p> <ul> <li>DNS: specified hostname</li> <li>IP: specified IP</li> </ul> <p>The generated certificate file is located at: <code>/var/lib/micado/zorp/config/ssl.pem</code></p> </li> </ul> <ul> <li> <p>The user-supplied option lets the user add the keypair as plain multiline strings (in unencrypted format) in the ansible_user_data.yml file under the 'cert' and 'key' subkeys respectively.</p> <p>Specify the default username and password for the administrative user in the <code>authentication</code> subtree.</p> </li> </ul> <p>Optionally you may use the Ansible Vault mechanism as described in Step 2 to protect the confidentiality and integrity of this file as well.</p>"},{"location":"install/ansible-install/#step-3b-optional-specify-credentials-to-use-private-docker-registries","title":"Step 3b: (Optional) Specify credentials to use private Docker registries","text":"<p>Set the Docker login credentials of your private Docker registry in which your private containers are stored. We recommend making a copy of our predefined template and edit it. MiCADO expects the docker registry credentials in a file, called credentials-docker-registry.yml. Please, do not modify the structure of the template!</p> <pre><code>cp sample-credentials-registries.yml credentials-registries.yml\nedit credentials-registries.yml\n</code></pre> <p>Edit the file according to the K3s documentation</p> <p>Optionally you may use the Ansible Vault mechanism as described in Step 2 to protect the confidentiality and integrity of this file as well.</p>"},{"location":"install/ansible-install/#step-4-launch-an-empty-cloud-vm-instance-for-micado-control-plane","title":"Step 4: Launch an empty cloud VM instance for MiCADO Control Plane","text":"<p>This new VM will host the MiCADO core services.</p> <p>a) Default port number for MiCADO service is <code>443</code>. Optionally, you can modify the port number stored by the variable called <code>web_listening_port</code> defined in the ansible config file called <code>project/host_vars/micado.yml</code>.</p> <p>b) Configure a cloud firewall settings which opens the following ports on the MiCADO Control Plane virtual machine:</p> Protocol Port(s) Service TCP 443* web listening port TCP 22 SSH TCP 6443 kube-apiserver TCP 10250 metrics UDP 51820 wireguard (Pod-Pod) <p>NOTE: <code>web listening port</code> should match with the actual value specified in Step 4a.</p> <p>c) Finally, launch the virtual machine with the proper settings (capacity, ssh keys, firewall): use any of aws, ec2, nova, etc command-line tools or web interface of your target cloud to launch a new VM. We recommend a VM with 2 cores, 4GB RAM, 20GB disk. Make sure you can ssh to it (password-free i.e.\u00a0ssh public key is deployed) and your user is able to sudo (to install MiCADO as root). Store its IP address which will be referred as <code>IP</code> in the following steps.</p>"},{"location":"install/ansible-install/#step-5-customize-the-inventory-file-for-the-micado-control-plane","title":"Step 5: Customize the inventory file for the MiCADO Control Plane","text":"<p>We recommend making a copy of our predefined template and edit it. Use the template inventory file, called sample-hosts.yml for customisation.</p> <pre><code>cd playbook/inventory\ncp sample-hosts.yml hosts.yml\nedit hosts.yml\n</code></pre> <p>Edit the <code>hosts.yml</code> file to set the variables. The following parameters under the key micado-target can be updated:</p> <ul> <li>ansible_host: specifies the publicly reachable ip address of the target machine where you intend to build/deploy a MiCADO Control Plane or build a MiCADO Worker. Set the public or floating <code>IP</code> of the master regardless the deployment method is remote or local. The ip specified here is used by the Dashboard for webpage redirection as well</li> <li>ansible_connection: specifies how the target host can be reached. Use \"ssh\" for remote or \"local\" for local installation. In case of remote installation, make sure you can authenticate yourself against MiCADO Control Plane. We recommend to deploy your public ssh key on MiCADO Control Plane before starting the deployment</li> <li>ansible_user: specifies the name of your sudoer account, defaults to \"ubuntu\"</li> <li>ansible_become: specifies if account change is needed to become root, defaults to \"True\"</li> <li>ansible_become_method: specifies which command to use to become superuser, defaults to \"sudo\"</li> <li>ansible_python_interpreter: specifies the interpreter to be used for ansible on the target host, defaults to \"/usr/bin/python3\"</li> </ul> <p>Please, revise all the parameters, however in most cases the default values are correct.</p>"},{"location":"install/ansible-install/#step-6-customize-the-deployment","title":"Step 6: Customize the deployment","text":"<p>A few parameters in project/host_vars/micado.yml can be fine tuned before deployment. They are as follows:</p> <ul> <li> <p>enable_optimizer: Setting this parameter to True enables the deployment of the Optimizer module, to perform more advanced scaling. Default is True.</p> </li> <li> <p>disable_worker_updates: Setting this parameter to False enables periodic software updates of the worker nodes. Default is True.</p> </li> <li> <p>grafana_admin_pwd: The string defined here will be the password for Grafana administrator.</p> </li> <li> <p>web_listening_port: Port number of the dasboard on MiCADO Control Plane. Default is 443.</p> </li> <li> <p>web_session_timeout: Timeout value in seconds for the Dashboard. Default is 600.</p> </li> <li> <p>enable_occopus: Install and enable Occopus for cloud orchestration. Default is True.</p> </li> <li> <p>enable_terraform: Install and enable Terraform for cloud orchestration. Default is False.</p> </li> </ul> <p>Note. MiCADO supports running both Occopus &amp; Terraform on the same Master, if desired</p>"},{"location":"install/ansible-install/#step-7-start-the-installation-of-micado-control-plane","title":"Step 7: Start the installation of MiCADO Control Plane","text":"<p>Run the following command to build and initalise a MiCADO Control Plane node on the empty VM you launched in Step 4 and pointed to in hosts.yml Step 5.</p> <pre><code>cd playbook/\nansible-playbook -i inventory/hosts.yml project/micado.yml\n</code></pre> <p>If you have used Vault to encrypt your credentials, you have to add the path to your vault credentials to the command line as described in the Ansible Vault documentation or provide it via command line using the command</p> <pre><code>cd playbook/\nansible-playbook -i inventory/hosts.yml project/micado.yml --ask-vault-pass\n</code></pre>"},{"location":"install/ansible-install/#optional-build-start-roles","title":"Optional: Build &amp; Start Roles","text":"<p>Optionally, you can split the deployment of your MiCADO Control Plane in two. The <code>build</code> tags prepare the node will all the necessary dependencies, libraries and images necessary for operation. The <code>start</code> tags intialise the cluster and all the MiCADO core components.</p> <p>You can clone the drive of a \"built\" MiCADO Control Plane (or otherwise make an image from it) to be reused again and again. This will greatly speed up the deployment of future instances of MiCADO.</p> <p>Running the following command will <code>build</code> a MiCADO Control Plane node on an empty Ubuntu VM.</p> <p><pre><code>ansible-playbook -i inventory/hosts.yml project/micado.yml --tags build\n</code></pre> You can then run the following command to <code>start</code> any \"built\" MiCADO Control Plane node which will initialise and launch the core components for operation.</p> <pre><code>ansible-playbook -i inventory/hosts.yml project/micado.yml --tags start\n</code></pre>"},{"location":"install/ansible-install/#advanced-cloud-specific-fixes","title":"Advanced: Cloud specific fixes","text":"<p>Certain cloud service providers may provide Virtual Machine images that are incompatible with the normal MiCADO installation. Where possible, we have included automated fixes for these, which can be applied using the <code>--tags</code> syntax of Ansible. See below for details:</p> <p>CloudSigma</p> <p>At the time of writing, the CloudSigma Ubuntu 20.04 virtual machine disk images are improperly configured, and SSL errors may appear during installation of MiCADO. A special task has been added to MiCADO to automate the fix when installing on CloudSigma instances.</p> <p>Simply use the following command instead of the command provided above. Notice the added tags</p> <pre><code>ansible-playbook -i inventory/hosts.yml project/micado.yml --tags all,cloudsigma\n</code></pre>"},{"location":"install/cli-install/","title":"CLI Install","text":"<p>For a straightforward approach to installing MiCADO on remote machines, especially for those unfamiliar with Ansible, we recommend using the command-line interface provided by <code>micado-client</code>.</p>"},{"location":"install/cli-install/#pre-requisites","title":"Pre-requisites","text":"<p>Provision or create a virtual machine with the following specification.</p> <p>Prepare a Linux-based system with Python, pip and SSH access to the above virtual machine.This could be a local device.</p>"},{"location":"install/cli-install/#install-micado-client","title":"Install micado-client","text":"<p>Use the Python package manager to install the latest MiCADO-Client, a Python library and command-line interface (CLI) for building and interacting with a MiCADO cluster.</p> <pre><code>pip install micado-client\n</code></pre>"},{"location":"install/cli-install/#collect-playbooks-and-configuration","title":"Collect Playbooks and configuration","text":"<p>The CLI will create a new directory, or populate an empty directory with Ansible Playbooks and configuration files for deploying MiCADO. Name this directory as you please.</p> <p>Tip</p> <p>If you are a sysadmin responsible for deploying multiple MiCADO clusters, you might name the directory according to the specific user or cloud you are preparing MiCADO for.</p> <p>By default, you will get the latest version of MiCADO. Ask for a specific version with the <code>\u2011\u2011version</code> flag.</p> latestv0.12.0 <pre><code>micado init jay_aws_micado\ncd jay_aws_micado\n</code></pre> <pre><code>micado init jay_aws_micado --version v0.12.0 cd jay_aws_micado\n</code></pre>"},{"location":"install/cli-install/#specify-the-configuration","title":"Specify the configuration","text":""},{"location":"install/cli-install/#configure-host-list","title":"Configure Host List","text":"<p>This command will open Ansible's inventory file in your preferred editor. The Ansible inventory points at the hosts to configure. You must provide SSH authentication details for the MiCADO Control Plane VM.</p> <pre><code>micado config hosts\n</code></pre> <p>For example, If you normally SSH to the VM with: <code>ssh ubuntu@123.456.78.90 -i /path/to/key</code></p> <p>Then your inventory file should read as below:</p> <pre><code>all:\nhosts:\nmicado:\nansible_host: 123.456.78.90\nansible_connection: ssh\nansible_user: ubuntu\nansible_ssh_private_key_file: /path/to/key\n</code></pre> <p>Tip</p> <p>If your SSH private key is at a standard location on your Ansible Control Node (e.g. <code>~/.ssh/id_rsa</code>) you may omit <code>ansible_ssh_private_key_file</code>.</p>"},{"location":"install/cli-install/#configure-cloud-credentials","title":"Configure Cloud Credentials","text":"This command will open MiCADO's cloud credential file in your preferred editor. Most of our clouds\u00a0are supported with username/password or key authentication. <pre><code>micado config cloud\n</code></pre> <p>Provide credentials for one or more clouds, as required. Unused clouds can be left blank.</p> <pre><code>resource:\n- type: ec2\nauth_data:\naccesskey: ABC123DEF\nsecretkey: 456XYZ789\n\n- type: cloudsigma\nauth_data:\nemail: password:\n\n- type: cloudbroker\nauth_data:\nemail: user@example.com\npassword: s3cur3_p4ssw0rd\n</code></pre> <p>Tip</p> <p>These can be updated on an existing cluster.</p> <p>The following clouds may require some additional explanation:</p> <ul> <li>OpenStack</li> <li>Google Cloud</li> <li>Microsoft Azure</li> <li>Oracle Cloud Infrastructure</li> </ul> Warning <p>This file will be stored as a Kubernetes secret on the MiCADO Control Plane. To keep it secure on your Ansible Control Node, we recommend you encrypt it with Ansible-Vault. We support only a single vault password. Remember to pass <code>\u2011\u2011vault</code> to <code>micado deploy</code>.</p>"},{"location":"install/cli-install/#configure-proxy-and-tls","title":"Configure Proxy and TLS","text":"<p>MiCADO proxies a dashboard for inspecting the cluster and the submitter endpoint for running application. This command configures basic auth and the x509 keypair used for TLS encryption of the dashboard and submitter.</p> <pre><code>micado config web\n</code></pre> <p>See below for examples of this file. The username // password login for the MiCADO Dashboard will, in both cases, be <code>admin</code> // <code>admin_Pass</code></p> Self SignedUser Supplied <pre><code>tls:\nprovision_method: self-signed\nauthentication:\nusername: admin\nemail: user@example.com\npassword: admin_Pass                 </code></pre> Info <p>The self-signed option generates a new keypair with the specified hostname as the subject/CN.This is <code>micado-master</code> by default, but can be configured with <code>micado config settings</code>.</p> <p>Two Subject Alternative Name (SAN) entries are also added:</p> <ul> <li>DNS: specified hostname</li> <li>IP: specified IP</li> </ul> <pre><code>tls:\nprovision_method: user-supplied\ncert: |\n-----BEGIN CERTIFICATE-----\nMIID0DCCArigAwIBAgIBATANBgkqhkiG9w0BAQUFADB/MQswCQYDVQQGEwJGUjET\n...\nkey: |\n-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEAvpnaPKLIKdvx98KW68lz8pGaRRcYersNGqPjpifMVjjE8LuC\n...\nauthentication:\nusername: admin\nemail: user@example.com\npassword: admin_Pass\n</code></pre> Warning <p>This file will be stored as a Kubernetes secret on the MiCADO Control Plane. To keep it secure on your Ansible Control Node, we recommend you encrypt it with Ansible-Vault. We support only a single vault password. Remember to pass <code>\u2011\u2011vault</code> to <code>micado deploy</code>.</p>"},{"location":"install/cli-install/#configure-container-registries","title":"Configure Container Registries","text":"<p>Container registry logins and mirrors are configured via the K3s <code>registries.yaml</code> file, which is very well documented at this link. The following command will open that file for editing.</p> <pre><code>micado config registry\n</code></pre> <p>This simple example provides username and password for the Docker private registry, but much more is possible.</p> <pre><code>configs:\nregistry-1.docker.io:\nauth:\nusername: USERNAME\npassword: PASSWORD\n</code></pre> <p>Tip</p> <p>These can be updated on an existing cluster.</p> Warning <p>This file will be stored as a Kubernetes secret on the MiCADO Control Plane. To keep it secure on your Ansible Control Node, we recommend you encrypt it with Ansible-Vault. We support only a single vault password. Remember to pass <code>\u2011\u2011vault</code> to <code>micado deploy</code>.</p>"},{"location":"install/cli-install/#configure-additional-settings","title":"Configure Additional Settings","text":"<p>Several additional options can be configured. The command below will open the additional settings file in your preferred editor.</p> <pre><code>micado config settings\n</code></pre> <code>web_listening_port</code> <p>integer. Port number of the dasboard on MiCADO Control Plane. Defaults to 443.</p> <code>enable_occopus</code> <p>boolean. Install and enable Occopus for cloud orchestration. Defaults to False.</p> <code>enable_terraform</code> <p>boolean. Install and enable Terraform for cloud orchestration. Defaults to True.</p> <code>enable_optimizer</code> <p>boolean. Setting this parameter to True enables the deployment of the Optimizer module, to perform more advanced scaling. Note this component is still in beta. Defaults to False.</p> <code>disable_worker_updates</code> <p>boolean. Setting this parameter to False enables periodic software updates of the worker nodes. Note this may have an adverse effect on worker node start times. Defaults to True.</p> <code>grafana_admin_pwd</code> <p>string. Configure the password for the Grafana administrator. Required for confiuring Grafana dashboards.</p> <code>web_session_timeout</code> <p>integer. Timeout value in seconds for the Dashboard. Defaults to 600.</p>"},{"location":"install/cli-install/#deploy-micado","title":"Deploy MiCADO","text":"<p>After everything has been configured, you can install MiCADO to the remote instance, which will launch the cluster.</p> <pre><code>micado up\n</code></pre>"},{"location":"install/quick-start/","title":"Quick-Start","text":"<p>This four step guide aims to get you started with MiCADO. For more detail on this installation method, please see CLI Install.</p>"},{"location":"install/quick-start/#pre-requisites","title":"Pre-requisites","text":"<p>First, provision a virtual machine in the cloud according to the requirements.</p> <p>The following commands can be run from any device or instance that has Python 3.8 or higher, and SSH access to the newly provisioned instance. This can be a local device, or remote instance.</p>"},{"location":"install/quick-start/#steps","title":"Steps","text":"<p>Example</p> <p>The command above will open the specified configuration in your preferred editor. Sample snippets of each config file are shown below.</p> hostscloudwebregistrysettings <p>Configure IP and username for SSH access to the control plane node. If your SSH private key is not at a default path (e.g. .ssh/id_rsa) also specify path here</p> <pre><code>all:\nhosts:\nmicado:\nansible_host: 123.456.78.90\nansible_connection: ssh\nansible_user: ubuntu\nansible_ssh_private_key_file: /path/to/private/key\n</code></pre> <p>Configure cloud credentials for desired clouds. Please consider using ansible-vault to encrypt this file</p> <pre><code>resource:\n- type: ec2\nauth_data:\naccesskey: ABC123DEF\nsecretkey: 456XYZ789\n</code></pre> <p>Configure login and TLS for the MiCADO Dashboard and Submitter. Please consider using ansible-vault to encrypt this file</p> <pre><code>tls:\nprovision_method: self-signed\nauthentication:\nusername: admin\nemail: user@example.com\npassword: s3cur3p4ssw0rd\n</code></pre> <p>Configure private registry mirror/auth details as a K3s registries file.</p> <pre><code>configs:\nregistry-1.docker.io:\nauth:\nusername: USERNAME\npassword: PASSWORD\n</code></pre> <p>Configure various advanced settings. See the relevant section for details.</p> <pre><code># enable specific components\n# -------------------------------------------------------\nenable_optimizer: False\nenable_occopus: False\nenable_terraform: True\n\n# enable multicloud support\n# -------------------------------------------------------\nenable_multicloud: False\n</code></pre> <p>That's it! Have a look at our demos to start deploying applications.</p>"},{"location":"install/quick-start/#install-micado-client","title":"Install <code>micado-client</code>","text":"<pre><code>pip install micado-client\n</code></pre>"},{"location":"install/quick-start/#initialise-a-new-config-directory","title":"Initialise a new config directory","text":"<p>The below example creates a new directory called <code>micado_conf_dir</code></p> <pre><code>micado init micado_conf_dir\ncd micado_conf_dir\n</code></pre>"},{"location":"install/quick-start/#configure-your-deployment","title":"Configure your deployment","text":"<p>It is mandatory to configure <code>hosts</code></p> hostscloudwebregistrysettings <pre><code>micado config hosts\n</code></pre> <pre><code>micado config cloud </code></pre> <pre><code>micado config web\n</code></pre> <pre><code>micado config registry\n</code></pre> <pre><code>micado config settings\n</code></pre>"},{"location":"install/quick-start/#deploy-the-micado-control-plane","title":"Deploy the MiCADO control plane","text":"<pre><code>micado up\n</code></pre>"},{"location":"install/requirements/","title":"Requirements","text":""},{"location":"install/requirements/#ansible-control-node","title":"Ansible Control Node","text":"<p>MiCADO is deployed by Ansible Playbooks - typically remotely via SSH. You will need a local device or other remote instance that has SSH access to the new virtual machine for the MiCADO control plane. The requirements for this Ansible control node are:</p> <ul> <li>SSH access to MiCADO control plane<ul> <li>Network access to port 22</li> <li>Private SSH key</li> </ul> </li> <li>Python 3.8 or higher<ul> <li><code>pip</code> is required for standard installation</li> </ul> </li> <li>Ansible 2.11 or higher</li> </ul> <p>Tip</p> <p>A correct version of Ansible is provided by the <code>micado-client</code> CLI recommended for installation, so installing it separately is usually not required.</p>"},{"location":"install/requirements/#micado-control-plane","title":"MiCADO Control Plane","text":"<p>The MiCADO control plane is a K3s server that runs a number of additional microservices providing additional functionality. See the minimum requirements below:</p> <ul> <li>x86_64</li> <li>Ubuntu 20.04 or 22.04</li> <li>2 vCPU</li> <li>4 GB RAM</li> <li>15GB Disk</li> <li>TCP Port <code>22</code> for installation via SSH</li> <li>TCP Port <code>443</code> for access to the WebUI</li> <li>TCP Ports <code>6443</code>, <code>10250</code> for cluster operation</li> <li>UDP Port <code>58120</code> for pod-to-pod communication</li> <li>Python 3.5 or higher</li> </ul> <p>You must configure SSH key-based authentication for this VM.</p>"},{"location":"install/requirements/#micado-cloud-workers","title":"MiCADO Cloud Workers","text":"<p>MiCADO supports provisoning workers on the following clouds. For best performance, keep the control plane and workers in the same cloud, region and availability zone.</p> Supported Clouds <ul> <li>Amazon EC2</li> <li>OpenStack Nova</li> <li>Microsoft Azure</li> <li>Google Cloud</li> <li>CloudSigma</li> <li>CloudBroker</li> </ul> <p>Worker requirements will vary based on your specific application requirements, but the minimum requirements are below:</p> <ul> <li>x86_64</li> <li>Ubuntu 20.04 or 22.04</li> <li>1 vCPU</li> <li>1 GB RAM</li> <li>5 GB Disk</li> <li>UDP Port <code>58120</code></li> </ul>"},{"location":"install/requirements/#micado-edge-workers","title":"MiCADO Edge Workers","text":"<p>Requirements at the edge are minimal, and are mostly dictated by KubeEdge.</p> <ul> <li>x86_64 or arm64/aarch64</li> <li>Most Linux distributions (tested on Ubuntu &amp; Raspbian)</li> <li>TCP Port 22 (SSH) for initial edge setup</li> </ul>"},{"location":"usage/","title":"Using MiCADO","text":"<p>Once your MiCADO cluster is prepared, you can:</p> <ul> <li>Visit the Dashboard</li> <li>Use the REST API</li> <li>Administrate the cluster</li> </ul> <p>To start running applications with MiCADO, you can:</p> <ul> <li>Play around with the demos</li> <li>Describe your application in an ADT</li> </ul>"},{"location":"usage/admin/","title":"Cluster Admin","text":"<p>Some common cluster admin functionalities are described below.</p>"},{"location":"usage/admin/#update-cloud-registry-logins","title":"Update Cloud &amp; Registry Logins","text":"<p>It is possible to modify the cloud and registry login credentials of an existing MiCADO cluster. Simply make the necessary changes to the appropriate credentials file (using <code>micado config</code>) and then run the following CLI command:</p> <pre><code>micado deploy --update-auth\n</code></pre>"},{"location":"usage/admin/#inspect-micado-components","title":"Inspect MiCADO Components","text":"<p>You can inspect the status and logs of MiCADO components via the Kubernetes Dashboard. Navigate to them by changing the namespace to <code>micado-system</code> or <code>micado-worker</code> and then viewing details of specific components in the Pods section</p> <p>You can also SSH into the MiCADO Control Plane and check the logs at any point after MiCADO is succesfully deployed. All logs are kept under <code>/var/log/micado</code> and are organised by components. Scaling decisions, for example, can be inspected under <code>/var/log/micado/policykeeper</code></p>"},{"location":"usage/admin/#accessing-user-defined-services","title":"Accessing user-defined services","text":"<p>In case your application contains a container exposing a service, you will have to ensure the following to access it.</p> <ul> <li>First set nodePort: xxxxx (where xxxxx is a port in range 30000-32767) in the properties: ports: TOSCA description of your docker container. More information on this in the ADT</li> <li>The container will be accessible at : . Both, the IP and the port values can be extracted from the Kubernetes Dashboard (in case you forget it). The IP can be found under Nodes &gt; my_micado_vm &gt; Addresses menu, while the port can be found under Discovery and load balancing &gt; Services &gt; my_app &gt; Internal endpoints menu."},{"location":"usage/cli/","title":"CLI","text":"<p>The same CLI used to install MiCADO can also be used to run applications described in MiCADO's Application Description Template (ADT) format. Both <code>YAML</code> and <code>CSAR</code> ADTs are supported.</p>"},{"location":"usage/cli/#commands","title":"Commands","text":""},{"location":"usage/cli/#run-an-app","title":"Run an app","text":"<pre><code>micado start FILENAME\n</code></pre>"},{"location":"usage/cli/#get-current-app-info","title":"Get current app info","text":"<pre><code>micado info\n</code></pre>"},{"location":"usage/cli/#delete-the-current-app","title":"Delete the current app","text":"<pre><code>micado stop\n</code></pre>"},{"location":"usage/dashboard/","title":"MiCADO Dashboard","text":"<p>MiCADO has a simple dashboard that collects several web-based user interfaces into a single view. To access the Dashboard, visit <code>https://[IP]:[PORT]</code>, where</p> <ul> <li> <p>[IP] is the ip address of the MiCADO Control Plane</p> </li> <li> <p>[PORT] is the <code>web_listening_port</code>, which defaults to 443, but can be configured with <code>micado config settings</code>.</p> </li> </ul>"},{"location":"usage/dashboard/#login","title":"Login","text":"<p>Login details are configured during install. If you opted for a self-signed certificate, you may have to bypass warnings generated by your browser. You should then be presented with a login screen where you can enter the username and password you configured.</p>"},{"location":"usage/dashboard/#interfaces","title":"Interfaces","text":"<p>The following interfaces are currently exposed:</p>"},{"location":"usage/dashboard/#kubernetes-dashboard","title":"Kubernetes Dashboard","text":"<p>A read-only instance of the Kubernetes WebUI providing a full overview of the infrastructure. Since MiCADO provides its own authentication, you can SKIP the authentication pop-up to gain access to the dashboard.</p> <p>By default, your application will be scheduled in the <code>default</code> Kubernetes namespace. The Kubernetes Dashboard is well-documented in the official docs.</p>"},{"location":"usage/dashboard/#grafana","title":"Grafana","text":"Graphically visualize the resources (nodes, containers) in time. After deploying your application, you can select the service whose metrics you want using the 'Service' drop down running above the graphs area."},{"location":"usage/dashboard/#prometheus","title":"Prometheus","text":"The monitoring subsystem. Recommended for developers, experts."},{"location":"usage/restapi/","title":"MiCADO Submitter RESTapi","text":"<p>MiCADO exposes a REST API for submitting and managing user applications. Please refer to the current OpenAPI specification.</p>"}]}